{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[2025-04-13 20:04:07,603] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sw/nix/store/r3bwp9b2501bv77y6g1nwkb483p0y9z2-cuda-12.3.2/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/mnt/sw/nix/store/r3bwp9b2501bv77y6g1nwkb483p0y9z2-cuda-12.3.2/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/mnt/sw/nix/store/r3bwp9b2501bv77y6g1nwkb483p0y9z2-cuda-12.3.2/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/mnt/sw/nix/store/r3bwp9b2501bv77y6g1nwkb483p0y9z2-cuda-12.3.2/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/mnt/sw/nix/store/r3bwp9b2501bv77y6g1nwkb483p0y9z2-cuda-12.3.2/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/mnt/sw/nix/store/r3bwp9b2501bv77y6g1nwkb483p0y9z2-cuda-12.3.2/lib64/libcufile.so: undefined reference to `shm_open'\n",
      "/mnt/sw/nix/store/r3bwp9b2501bv77y6g1nwkb483p0y9z2-cuda-12.3.2/lib64/libcufile.so: undefined reference to `shm_unlink'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import importlib\n",
    "import structures\n",
    "importlib.reload(structures)\n",
    "from structures import *\n",
    "import losses\n",
    "importlib.reload(losses)\n",
    "from losses import *\n",
    "import trainer\n",
    "importlib.reload(trainer)\n",
    "from trainer import *\n",
    "#dtype = torch.float32\n",
    "dtype = torch.double\n",
    "torch.set_default_dtype(dtype)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# Set the device to CUDA if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "import deepspeed\n",
    "import random\n",
    "sys.path.append(\"/mnt/home/yjo10/ceph/myutils\")\n",
    "import plt_utils as pu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([590, 48])\n"
     ]
    }
   ],
   "source": [
    "def normalizer(_data):\n",
    "    _max = torch.abs(_data.max())\n",
    "    data_max = torch.ones(_data.size(), device=device)*_max\n",
    "    _data = _data/data_max\n",
    "    return _data, _max\n",
    "    \n",
    "def denormalizer(_data, _max):\n",
    "    data_max = torch.ones(_data.size(), device=device)*_max\n",
    "    return _data*data_max\n",
    "\n",
    "# Example unsupervised data: 100 samples with 10 features each\n",
    "data = np.load(\"../data/two_body_train_data_new.npy\")\n",
    "num_samples = data.shape[0]\n",
    "num_samples_start = int(data.shape[0]*0.8)\n",
    "num_samples_start = 0\n",
    "num_samples_end   = data.shape[0]\n",
    "num_samples = num_samples_end - num_samples_start\n",
    "num_features = data.shape[1]\n",
    "\n",
    "# Placeholder for input (magnitudes of velocities and accelerations)\n",
    "data_org = torch.tensor(data[num_samples_start:num_samples_end,:], dtype=dtype).to(device)\n",
    "data = torch.empty((num_samples,6+num_features), dtype=dtype).to(device)\n",
    "data[:,6:] = data_org\n",
    "\n",
    "\n",
    "# Magnitudes vel, acc, mass\n",
    "temp, tmax = normalizer(data_org[:,4:7])\n",
    "vel = denormalizer(torch.norm(temp, p=2, dim=1),tmax)\n",
    "acc = torch.empty((num_samples,4), dtype=dtype).to(device)\n",
    "\n",
    "temp,tmax = normalizer(data_org[:,7:10])\n",
    "acc[:,0] = denormalizer(torch.norm(temp, p=2, dim=1),tmax)\n",
    "temp,tmax = normalizer(data_org[:,10:13])\n",
    "acc[:,1] = denormalizer(torch.norm(temp, p=2, dim=1),tmax)\n",
    "temp,tmax = normalizer(data_org[:,13:16])\n",
    "acc[:,2] = denormalizer(torch.norm(temp, p=2, dim=1),tmax)\n",
    "temp,tmax = normalizer(data_org[:,16:19])\n",
    "acc[:,3] = denormalizer(torch.norm(temp, p=2, dim=1),tmax)\n",
    "mass = data_org[:,0]\n",
    "\n",
    "data[:,0] = vel\n",
    "data[:,1:5] = acc\n",
    "data[:,5] = mass\n",
    "\n",
    "# Normalize the data\n",
    "eps = 1e-5\n",
    "data_min = data[:,:6].min(axis=0, keepdim=True).values\n",
    "data_max = data[:,:6].max(axis=0, keepdim=True).values\n",
    "data[:,:6] = (data[:,:6] - data_min) / (data_max - data_min)+eps\n",
    "\n",
    "\n",
    "# Wrap the feature tensor in a TensorDataset\n",
    "# Each item from the dataset will be a tuple containing one tensor (X[i],)\n",
    "dataset = TensorDataset(data)\n",
    "\n",
    "# Define the proportion for the test set (e.g., 20%)\n",
    "test_ratio = 0.2\n",
    "num_test = int(num_samples * test_ratio)\n",
    "num_train = num_samples - num_test\n",
    "\n",
    "# Use random_split to split the dataset into train and test subsets\n",
    "# Define generator with a fixed seed\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, test_dataset = random_split(dataset, [num_train, num_test], generator=generator)\n",
    "\n",
    "# Create DataLoaders for the training and testing datasets\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-11 15:57:08,304] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown\n",
      "[2025-04-11 15:57:08,305] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1\n",
      "[2025-04-11 15:57:08,307] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-04-11 15:57:08,308] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2025-04-11 15:57:08,308] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-04-11 15:57:08,309] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2025-04-11 15:57:08,309] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
      "[2025-04-11 15:57:08,309] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 1 optimizer\n",
      "[2025-04-11 15:57:08,310] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000\n",
      "[2025-04-11 15:57:08,310] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000\n",
      "[2025-04-11 15:57:08,311] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2025-04-11 15:57:08,311] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2025-04-11 15:57:08,464] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-04-11 15:57:08,465] [INFO] [utils.py:782:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.02 GB         Max_CA 0 GB \n",
      "[2025-04-11 15:57:08,466] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 61.93 GB, percent = 6.1%\n",
      "[2025-04-11 15:57:08,606] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-04-11 15:57:08,607] [INFO] [utils.py:782:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.02 GB         Max_CA 0 GB \n",
      "[2025-04-11 15:57:08,607] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 61.93 GB, percent = 6.1%\n",
      "[2025-04-11 15:57:08,608] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized\n",
      "[2025-04-11 15:57:08,747] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-04-11 15:57:08,748] [INFO] [utils.py:782:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.02 GB         Max_CA 0 GB \n",
      "[2025-04-11 15:57:08,749] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 61.93 GB, percent = 6.1%\n",
      "[2025-04-11 15:57:08,759] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-04-11 15:57:08,760] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-04-11 15:57:08,760] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-04-11 15:57:08,761] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[[0.9, 0.999]]\n",
      "[2025-04-11 15:57:08,761] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:\n",
      "[2025-04-11 15:57:08,761] [INFO] [config.py:1005:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-04-11 15:57:08,762] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-04-11 15:57:08,762] [INFO] [config.py:1005:print]   amp_enabled .................. False\n",
      "[2025-04-11 15:57:08,762] [INFO] [config.py:1005:print]   amp_params ................... False\n",
      "[2025-04-11 15:57:08,763] [INFO] [config.py:1005:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-04-11 15:57:08,763] [INFO] [config.py:1005:print]   bfloat16_enabled ............. False\n",
      "[2025-04-11 15:57:08,763] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-04-11 15:57:08,764] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-04-11 15:57:08,764] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-04-11 15:57:08,765] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-04-11 15:57:08,765] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1552a5fcb550>\n",
      "[2025-04-11 15:57:08,765] [INFO] [config.py:1005:print]   communication_data_type ...... None\n",
      "[2025-04-11 15:57:08,766] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-04-11 15:57:08,766] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False\n",
      "[2025-04-11 15:57:08,767] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False\n",
      "[2025-04-11 15:57:08,767] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-04-11 15:57:08,767] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False\n",
      "[2025-04-11 15:57:08,767] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False\n",
      "[2025-04-11 15:57:08,768] [INFO] [config.py:1005:print]   disable_allgather ............ False\n",
      "[2025-04-11 15:57:08,768] [INFO] [config.py:1005:print]   dump_state ................... False\n",
      "[2025-04-11 15:57:08,768] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-04-11 15:57:08,769] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False\n",
      "[2025-04-11 15:57:08,769] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-04-11 15:57:08,769] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-04-11 15:57:08,770] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-04-11 15:57:08,770] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-04-11 15:57:08,771] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-04-11 15:57:08,771] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-04-11 15:57:08,771] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False\n",
      "[2025-04-11 15:57:08,772] [INFO] [config.py:1005:print]   elasticity_enabled ........... False\n",
      "[2025-04-11 15:57:08,772] [INFO] [config.py:1005:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-04-11 15:57:08,772] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None\n",
      "[2025-04-11 15:57:08,773] [INFO] [config.py:1005:print]   fp16_enabled ................. False\n",
      "[2025-04-11 15:57:08,773] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-04-11 15:57:08,773] [INFO] [config.py:1005:print]   global_rank .................. 0\n",
      "[2025-04-11 15:57:08,774] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None\n",
      "[2025-04-11 15:57:08,774] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 1\n",
      "[2025-04-11 15:57:08,774] [INFO] [config.py:1005:print]   gradient_clipping ............ 0.0\n",
      "[2025-04-11 15:57:08,774] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-04-11 15:57:08,775] [INFO] [config.py:1005:print]   graph_harvesting ............. False\n",
      "[2025-04-11 15:57:08,775] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-04-11 15:57:08,775] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 65536\n",
      "[2025-04-11 15:57:08,776] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False\n",
      "[2025-04-11 15:57:08,777] [INFO] [config.py:1005:print]   loss_scale ................... 0\n",
      "[2025-04-11 15:57:08,777] [INFO] [config.py:1005:print]   memory_breakdown ............. False\n",
      "[2025-04-11 15:57:08,777] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False\n",
      "[2025-04-11 15:57:08,777] [INFO] [config.py:1005:print]   mics_shard_size .............. -1\n",
      "[2025-04-11 15:57:08,778] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-04-11 15:57:08,778] [INFO] [config.py:1005:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-04-11 15:57:08,779] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-04-11 15:57:08,779] [INFO] [config.py:1005:print]   optimizer_name ............... adam\n",
      "[2025-04-11 15:57:08,779] [INFO] [config.py:1005:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.9, 0.999], 'eps': 1e-08}\n",
      "[2025-04-11 15:57:08,780] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-04-11 15:57:08,780] [INFO] [config.py:1005:print]   pld_enabled .................. False\n",
      "[2025-04-11 15:57:08,780] [INFO] [config.py:1005:print]   pld_params ................... False\n",
      "[2025-04-11 15:57:08,780] [INFO] [config.py:1005:print]   prescale_gradients ........... False\n",
      "[2025-04-11 15:57:08,781] [INFO] [config.py:1005:print]   scheduler_name ............... None\n",
      "[2025-04-11 15:57:08,781] [INFO] [config.py:1005:print]   scheduler_params ............. None\n",
      "[2025-04-11 15:57:08,781] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-04-11 15:57:08,782] [INFO] [config.py:1005:print]   sparse_attention ............. None\n",
      "[2025-04-11 15:57:08,782] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False\n",
      "[2025-04-11 15:57:08,782] [INFO] [config.py:1005:print]   steps_per_print .............. None\n",
      "[2025-04-11 15:57:08,782] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-04-11 15:57:08,783] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-04-11 15:57:08,783] [INFO] [config.py:1005:print]   train_batch_size ............. 8\n",
      "[2025-04-11 15:57:08,783] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  8\n",
      "[2025-04-11 15:57:08,784] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False\n",
      "[2025-04-11 15:57:08,784] [INFO] [config.py:1005:print]   use_node_local_storage ....... False\n",
      "[2025-04-11 15:57:08,785] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False\n",
      "[2025-04-11 15:57:08,785] [INFO] [config.py:1005:print]   weight_quantization_config ... None\n",
      "[2025-04-11 15:57:08,786] [INFO] [config.py:1005:print]   world_size ................... 1\n",
      "[2025-04-11 15:57:08,786] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False\n",
      "[2025-04-11 15:57:08,787] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-04-11 15:57:08,787] [INFO] [config.py:1005:print]   zero_enabled ................. True\n",
      "[2025-04-11 15:57:08,787] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-04-11 15:57:08,788] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1\n",
      "[2025-04-11 15:57:08,788] [INFO] [config.py:991:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.001, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08\n",
      "        }\n",
      "    }, \n",
      "    \"fp32\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 1\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_294660/1023649848.py:259: RuntimeWarning: invalid value encountered in log10\n",
      "  print(f\"Epoch [{epoch}/{num_epochs}], Timestep error: {train_res['time_step_relative_error']:.4e}/{val_res['time_step_relative_error']:.4e}, Energy Loss: {np.log10(train_res['energy_error']):.4e}/{np.log10(train_res['energy_error_fiducial']):.4e}, {np.log10(val_res['energy_error']):.4e}/{np.log10(val_res['energy_error_fiducial']):.4e}, Time step: {train_res['time_step']:.4e}/{train_res['time_step_fiducial']:.4e}, {val_res['time_step']:.4e}/{val_res['time_step_fiducial']:.4e}, {val_res['time_step_std']:.4e}/{val_res['time_step_fiducial_std']:.4e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1000], Timestep error: 7.4384e+06/4.8365e+06, Energy Loss: 1.0789e+00/nan, 1.0444e+00/nan, Time step: 2.1187e+00/1.6605e-03, 1.4588e+00/1.7660e-03, 3.5109e-02/2.3513e-03\n",
      "Epoch [1/1000], Timestep error: 3.4769e+06/5.4943e+05, Energy Loss: 1.0075e+00/nan, 8.8665e-01/nan, Time step: 8.3408e-01/1.6605e-03, 2.2308e-01/1.7660e-03, 7.6768e-02/2.3513e-03\n",
      "Epoch [2/1000], Timestep error: 2.5004e+05/7.8335e+02, Energy Loss: 5.6112e-01/nan, nan/nan, Time step: 5.5184e-02/1.6605e-03, 2.0388e-03/1.7660e-03, 3.2613e-03/2.3513e-03\n",
      "Epoch [3/1000], Timestep error: 2.1264e+03/2.3032e+01, Energy Loss: nan/nan, nan/nan, Time step: 6.6371e-04/1.6605e-03, 2.3838e-04/1.7660e-03, 4.6830e-04/2.3513e-03\n",
      "Epoch [4/1000], Timestep error: 5.5202e+02/1.2012e+01, Energy Loss: nan/nan, nan/nan, Time step: 1.8123e-04/1.6605e-03, 1.5257e-04/1.7660e-03, 3.0615e-04/2.3513e-03\n",
      "Epoch [5/1000], Timestep error: 3.4472e+02/1.1379e+01, Energy Loss: nan/nan, nan/nan, Time step: 1.3086e-04/1.6605e-03, 1.4248e-04/1.7660e-03, 2.8566e-04/2.3513e-03\n",
      "--> Best model saved at epoch 6 with val loss 4.7689e+01\n",
      "--> Best model saved at epoch 6 with largest timestep 1.7056e-04\n",
      "Epoch [6/1000], Timestep error: 3.6781e+02/1.6097e+01, Energy Loss: nan/nan, nan/nan, Time step: 1.4387e-04/1.6605e-03, 1.7056e-04/1.7660e-03, 3.3608e-04/2.3513e-03\n",
      "Epoch [7/1000], Timestep error: 4.1013e+02/7.9376e+00, Energy Loss: nan/nan, nan/nan, Time step: 1.6200e-04/1.6605e-03, 1.0500e-04/1.7660e-03, 2.1190e-04/2.3513e-03\n",
      "Epoch [8/1000], Timestep error: 2.5490e+02/1.0664e+01, Energy Loss: nan/nan, nan/nan, Time step: 1.0726e-04/1.6605e-03, 1.2405e-04/1.7660e-03, 2.4688e-04/2.3513e-03\n",
      "Epoch [9/1000], Timestep error: 2.5001e+02/4.6404e+00, Energy Loss: nan/nan, nan/nan, Time step: 1.3143e-04/1.6605e-03, 7.1253e-05/1.7660e-03, 1.4618e-04/2.3513e-03\n",
      "Epoch [10/1000], Timestep error: 1.4071e+02/7.2986e+00, Energy Loss: nan/nan, nan/nan, Time step: 6.8531e-05/1.6605e-03, 9.7760e-05/1.7660e-03, 1.9721e-04/2.3513e-03\n",
      "Epoch [11/1000], Timestep error: 2.6831e+02/1.0727e+01, Energy Loss: nan/nan, nan/nan, Time step: 1.1270e-04/1.6605e-03, 1.2354e-04/1.7660e-03, 2.4526e-04/2.3513e-03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[16], line 91\u001b[0m\n",
      "\u001b[1;32m     85\u001b[0m lists \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:[], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:[], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_energy\u001b[39m\u001b[38;5;124m\"\u001b[39m:[], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_energy\u001b[39m\u001b[38;5;124m\"\u001b[39m:[], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_time_step\u001b[39m\u001b[38;5;124m\"\u001b[39m:[], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_time_step\u001b[39m\u001b[38;5;124m\"\u001b[39m:[]}\n",
      "\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n",
      "\u001b[1;32m     88\u001b[0m \n",
      "\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m#train_loss, train_energy = \\\u001b[39;00m\n",
      "\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m#    train_one_epoch(model, optimizer, criterion, train_loader, input_mask, weights, device)\u001b[39;00m\n",
      "\u001b[0;32m---> 91\u001b[0m     train_res \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch_deepspeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_engine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m#test_loss, energy_error, energy_error_std, energy_error_fiducial, energy_error_fiducial_std, energy_pred, energy_init, time_step, time_step_fiducial = \\\u001b[39;00m\n",
      "\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m#    validate(model, criterion, test_loader, input_mask, weights, device)\u001b[39;00m\n",
      "\u001b[1;32m     95\u001b[0m     val_res \u001b[38;5;241m=\u001b[39m validate_deepspeed(model_engine, criterion, test_loader, input_mask, weights, device)\n",
      "\n",
      "File \u001b[0;32m~/projects/AITimeStepper/jupyter_notebooks/../src/trainer.py:18\u001b[0m, in \u001b[0;36mtrain_one_epoch_deepspeed\u001b[0;34m(model_engine, optimizer, criterion, train_loader, input_mask, weights, device)\u001b[0m\n",
      "\u001b[1;32m     13\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X,) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n",
      "\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#optimizer.zero_grad()       # Clear gradients\u001b[39;00m\n",
      "\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m#print(X.shape)\u001b[39;00m\n",
      "\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#print(X[:,:12].shape)\u001b[39;00m\n",
      "\u001b[0;32m---> 18\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#print(output)\u001b[39;00m\n",
      "\u001b[1;32m     20\u001b[0m     loss, loss_terms \u001b[38;5;241m=\u001b[39m criterion(model_engine, output, X, weights)  \u001b[38;5;66;03m# Compute loss between output and input\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/pyenv/torch/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:18\u001b[0m, in \u001b[0;36minstrument_w_nvtx.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enable_nvtx:\n",
      "\u001b[1;32m     17\u001b[0m     get_accelerator()\u001b[38;5;241m.\u001b[39mrange_push(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m)\n",
      "\u001b[0;32m---> 18\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enable_nvtx:\n",
      "\u001b[1;32m     20\u001b[0m     get_accelerator()\u001b[38;5;241m.\u001b[39mrange_pop()\n",
      "\n",
      "File \u001b[0;32m~/pyenv/torch/lib/python3.10/site-packages/deepspeed/runtime/engine.py:1987\u001b[0m, in \u001b[0;36mDeepSpeedEngine.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_auto_cast():\n",
      "\u001b[1;32m   1985\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cast_inputs_half(inputs)\n",
      "\u001b[0;32m-> 1987\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_optimization_partition_weights():\n",
      "\u001b[1;32m   1990\u001b[0m     \u001b[38;5;66;03m# Disable automated discovery of external parameters\u001b[39;00m\n",
      "\u001b[1;32m   1991\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mmodules():\n",
      "\n",
      "File \u001b[0;32m/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/projects/AITimeStepper/jupyter_notebooks/../src/structures.py:78\u001b[0m, in \u001b[0;36mFullyConnectedNN.forward\u001b[0;34m(self, x)\u001b[0m\n",
      "\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n",
      "\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\n",
      "File \u001b[0;32m/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/site-packages/torch/fx/traceback.py:68\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n",
      "\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\n",
      "File \u001b[0;32m/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/traceback.py:227\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n",
      "\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    226\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n",
      "\u001b[0;32m--> 227\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    228\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n",
      "\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "\n",
      "File \u001b[0;32m/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/traceback.py:379\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n",
      "\u001b[1;32m    376\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n",
      "\u001b[1;32m    377\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n",
      "\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n",
      "\u001b[0;32m--> 379\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n",
      "\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "\n",
      "File \u001b[0;32m/mnt/sw/nix/store/71ksmx7k6xy3v9ksfkv5mp5kxxp64pd6-python-3.10.13-view/lib/python3.10/linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n",
      "\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n",
      "\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[1;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import structures\n",
    "importlib.reload(structures)\n",
    "from structures import *\n",
    "import losses\n",
    "importlib.reload(losses)\n",
    "from losses import *\n",
    "import trainer\n",
    "importlib.reload(trainer)\n",
    "from trainer import *\n",
    "\n",
    "input_mask = np.r_[0:6]\n",
    "input_size = len(input_mask)\n",
    "#hidden_dims = [32,64,64,32]     # Number of hidden neurons\n",
    "#hidden_dims = [8,16,16,4]     # Number of hidden neurons\n",
    "hidden_dims = [12,12,6]     # Number of hidden neurons\n",
    "#hidden_dims = [16,128,32,8]     # Number of hidden neurons\n",
    "#hidden_dims = [8,8]     # Number of hidden neurons\n",
    "output_size = 2     # Number of output \n",
    "TargetEnergyError = 1e-3\n",
    "weights = {\"time_step\":10, \"energy_loss\":1}\n",
    "\n",
    "# Instantiate the model\n",
    "try: \n",
    "    del mode\n",
    "    l\n",
    "    print(\"model deleted\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "ds_config = {\n",
    "    \"train_batch_size\": batch_size,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"Adam\",\n",
    "        \"params\": {\"lr\": 1e-3, \"betas\": [0.9, 0.999], \"eps\": 1e-8}\n",
    "    },\n",
    "    \"fp32\": {\n",
    "        \"enabled\": True  # Enables mixed precision training\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 1  # Enable ZeRO Stage 1 for memory optimization\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "#model = SimpleNN(input_size, hidden_size, output_size).to(device)\n",
    "model1 = FullyConnectedNN(input_dim=input_size, output_dim=output_size, hidden_dims=hidden_dims, activation='relu', dropout=0.0, output_positive=False).to(device)\n",
    "model2 = FullyConnectedNN(input_dim=input_size, output_dim=output_size, hidden_dims=hidden_dims, activation='relu', dropout=0.0, output_positive=False).to(device)\n",
    "model3 = FullyConnectedNN(input_dim=input_size, output_dim=output_size, hidden_dims=hidden_dims, activation='relu', dropout=0.0, output_positive=False).to(device)\n",
    "\n",
    "\n",
    "model_engine, optimizer, _, _ = deepspeed.initialize(\n",
    "    model=model,\n",
    "    model_parameters=model.parameters(),\n",
    "    config=ds_config\n",
    ")\n",
    "\n",
    "# Define the loss function (CrossEntropyLoss is common for classification tasks)\n",
    "criterion = CustomizableLoss3DM(nParticle=2, nAttribute=20, nBatch=batch_size,alpha=weights['time_step'], beta=weights['energy_loss'], gamma=weights['energy_loss'], TargetEnergyError=TargetEnergyError,\n",
    "                            data_min=data_min, data_max=data_max,device=device)\n",
    "\n",
    "# Define the optimizer (Stochastic Gradient Descent in this example)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=0.00001)\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=0.00001)\n",
    "\n",
    "\n",
    "\n",
    "# Paths to save best and last models\n",
    "base_path = \"/mnt/home/yjo10/projects/AITimeStepper/data/models/two_body/\"\n",
    "best_val_path = base_path + \"best_model_loss.pth\"\n",
    "best_timestep_path = base_path + \"best_model_timestep.pth\"\n",
    "#last_model_path = base_path + f\"last_model_{epoch}.pth\"\n",
    "best_val_loss = float(\"inf\")\n",
    "best_largest_timestep = 0\n",
    "with open(base_path+\"c++/normalization_factors.txt\", \"w\") as f:\n",
    "    f.write(f\"{data_min[0][0]} {data_min[0][1]} {data_min[0][2]} {data_min[0][3]} {data_min[0][4]} {data_min[0][5]}\\n\")    \n",
    "    f.write(f\"{data_max[0][0]} {data_max[0][1]} {data_max[0][2]} {data_max[0][3]} {data_max[0][4]} {data_max[0][5]}\\n\")    \n",
    "\n",
    "example_input = torch.randn(1, input_size).to(device)\n",
    "exp_time_step = 1\n",
    "exp_energy_loss = 1\n",
    "# Training routine\n",
    "num_epochs = 1000\n",
    "lists = {\"training_loss\":[], \"val_loss\":[], \"training_energy\":[], \"val_energy\":[], \"training_time_step\":[], \"val_time_step\":[]}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #train_loss, train_energy = \\\n",
    "    #    train_one_epoch(model, optimizer, criterion, train_loader, input_mask, weights, device)\n",
    "    train_res = train_one_epoch_deepspeed(model_engine, optimizer, criterion, train_loader, input_mask, weights, device)\n",
    "        \n",
    "    #test_loss, energy_error, energy_error_std, energy_error_fiducial, energy_error_fiducial_std, energy_pred, energy_init, time_step, time_step_fiducial = \\\n",
    "    #    validate(model, criterion, test_loader, input_mask, weights, device)\n",
    "    val_res = validate_deepspeed(model_engine, criterion, test_loader, input_mask, weights, device)\n",
    "\n",
    "    lists[\"training_loss\"].append(train_res[\"train_loss\"])\n",
    "    lists[\"training_energy\"].append(train_res[\"energy_error\"])\n",
    "    lists[\"training_time_step\"].append(train_res[\"time_step\"])\n",
    "\n",
    "    lists[\"val_loss\"].append(val_res[\"val_loss\"])\n",
    "    lists[\"val_energy\"].append(val_res[\"energy_error\"])\n",
    "    lists[\"val_time_step\"].append(val_res[\"time_step\"])\n",
    "\n",
    "    if epoch > 5:\n",
    "        # Save the best model based on validation loss\n",
    "        if val_res['val_loss'] < best_val_loss:\n",
    "            best_val_loss = val_res['val_loss']\n",
    "            torch.save(model.state_dict(), best_val_path)\n",
    "            #traced_model = torch.jit.trace(model, example_input)\n",
    "            traced_model = torch.jit.script(model)\n",
    "            traced_model.save(base_path + f\"c++/best_model_loss.pt\")\n",
    "            print(f\"--> Best model saved at epoch {epoch} with val loss {best_val_loss:.4e}\")\n",
    "        \n",
    "        # Save the best model based on validation largest timestep\n",
    "        if val_res['time_step'] > best_largest_timestep:\n",
    "            best_largest_timestep = val_res['time_step']\n",
    "            torch.save(model.state_dict(), best_timestep_path)\n",
    "            #traced_model = torch.jit.trace(model, example_input)\n",
    "            traced_model = torch.jit.script(model)\n",
    "            traced_model.save(base_path + f\"c++/best_model_timestep.pt\")\n",
    "            print(f\"--> Best model saved at epoch {epoch} with largest timestep {best_largest_timestep:.4e}\")\n",
    "\n",
    "    # Save the last model after every epoch (or just once at the end)\n",
    "    torch.save(model.state_dict(), base_path + f\"model_{epoch}.pth\")\n",
    "    #traced_model = torch.jit.trace(model, example_input)\n",
    "    traced_model = torch.jit.script(model)\n",
    "    traced_model.save(base_path + f\"c++/model_{epoch}.pt\")\n",
    "\n",
    "\n",
    "\n",
    "    fig, axes = pu.generateAxesForMultiplePlots(shape=(1,3),figsize=(10,10),hspace=0.1,wspace=0.1,\n",
    "                                    gridspec=None)\n",
    "    for i, (key, value) in enumerate(lists.items()):\n",
    "        y = i//2\n",
    "        axes[y].plot(range(epoch+1), value, label=key)\n",
    "        axes[y].scatter(range(epoch+1), value)\n",
    "    axes[0].set_yscale(\"log\")\n",
    "    axes[2].set_yscale(\"log\")\n",
    "\n",
    "    axes[0].grid()\n",
    "    axes[1].grid()\n",
    "    axes[2].grid()\n",
    "\n",
    "\n",
    "    axes[2].axhline(y=val_res['time_step_fiducial'], c='r',alpha=0.5,zorder=-1,label=\"Timestep Fiducial\")\n",
    "    axes[1].axhline(y=np.log10(TargetEnergyError), c='r',alpha=0.5,zorder=-1,label=\"Target energy\")\n",
    "    #plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0.)\n",
    "    axes[0].legend()\n",
    "    axes[1].legend()\n",
    "    axes[2].legend()\n",
    "    #plt.tight_layout()\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[1].set_ylabel(\"log(Relative Error of Energy)\")\n",
    "    axes[2].set_ylabel(\"Timestep\")\n",
    "    axes[2].set_xlabel(\"epoch\")\n",
    "    plt.savefig(\"learning_curve.png\",dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    fig, axes = pu.generateAxesForMultiplePlots(shape=(1,2),figsize=(10,15),hspace=0.1,wspace=0.1,\n",
    "                                    gridspec=None)\n",
    "    # Get one batch from the validation loader\n",
    "    dataiter = iter(train_loader)\n",
    "    inputs, = next(dataiter)\n",
    "\n",
    "    # Determine how many samples to take (if batch size < 50, take the whole batch)\n",
    "    num_samples = min(50, batch_size)\n",
    "\n",
    "    # Randomly select indices from the batch\n",
    "    indices = random.sample(range(batch_size), num_samples)\n",
    "    sampled_inputs = inputs[:,input_mask][indices]\n",
    "    sampled_targets = inputs[indices,25]\n",
    "\n",
    "    # Compute model predictions with no gradient computation\n",
    "    with torch.no_grad():\n",
    "        sampled_predictions = model(sampled_inputs)\n",
    "\n",
    "    # Convert tensors to numpy arrays for plotting\n",
    "    sampled_predictions = sampled_predictions[:,0].cpu().numpy().flatten()\n",
    "    sampled_targets = sampled_targets.cpu().numpy().flatten()\n",
    "\n",
    "    # Create sample indices for the x-axis (1st sample, 2nd sample, etc.)\n",
    "    sample_indices = list(range(1, num_samples + 1))\n",
    "\n",
    "    # Plot predictions and ground truth vs sample number\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # Add vertical lines for each data point\n",
    "    for x in sample_indices:\n",
    "        axes[0].axvline(x=x, color='grey', linestyle='-', alpha=0.2,zorder=0)\n",
    "    axes[0].scatter(sample_indices, sampled_targets, marker='o', label='Ground Truth', fc=\"none\", ec=\"k\",zorder=10)\n",
    "    axes[0].scatter(sample_indices, np.power(10,sampled_predictions), marker='s', label='Predictions', fc=\"none\",ec=\"r\" ,zorder=10)\n",
    "    axes[0].set_yscale(\"log\")\n",
    "    axes[0].set_xlabel('Sample Number')\n",
    "    axes[0].set_ylabel('Value')\n",
    "\n",
    "    # Get one batch from the validation loader\n",
    "    dataiter = iter(test_loader)\n",
    "    inputs, = next(dataiter)\n",
    "\n",
    "    # Determine how many samples to take (if batch size < 50, take the whole batch)\n",
    "    num_samples = min(50, batch_size)\n",
    "\n",
    "    # Randomly select indices from the batch\n",
    "    indices = random.sample(range(batch_size), num_samples)\n",
    "    sampled_inputs = inputs[:,input_mask][indices]\n",
    "    sampled_targets = inputs[indices,25]\n",
    "\n",
    "    # Compute model predictions with no gradient computation\n",
    "    with torch.no_grad():\n",
    "        sampled_predictions = model(sampled_inputs)\n",
    "\n",
    "    # Convert tensors to numpy arrays for plotting\n",
    "    sampled_predictions = sampled_predictions[:,0].cpu().numpy().flatten()\n",
    "    sampled_targets = sampled_targets.cpu().numpy().flatten()\n",
    "\n",
    "    # Create sample indices for the x-axis (1st sample, 2nd sample, etc.)\n",
    "    sample_indices = list(range(1, num_samples + 1))\n",
    "\n",
    "    # Plot predictions and ground truth vs sample number\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # Add vertical lines for each data point\n",
    "    for x in sample_indices:\n",
    "        axes[1].axvline(x=x, color='grey', linestyle='-', alpha=0.2,zorder=0)\n",
    "    axes[1].scatter(sample_indices, sampled_targets, marker='o', label='Ground Truth', fc=\"none\", ec=\"k\",zorder=10)\n",
    "    axes[1].scatter(sample_indices, np.power(10,sampled_predictions), marker='s', label='Predictions', fc=\"none\",ec=\"r\" ,zorder=10)\n",
    "    axes[1].set_yscale(\"log\")\n",
    "    axes[1].set_xlabel('Sample Number')\n",
    "    axes[1].set_ylabel('Value')\n",
    "\n",
    "    axes[0].set_title(f'Epoch [{epoch}/{num_epochs}] Predictions and Ground Truth vs. Sample Number')\n",
    "    #plt.legend()\n",
    "    axes[0].grid()\n",
    "    axes[1].grid()\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.savefig(\"samples.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    if time_step < time_step_fiducial:\n",
    "        weights['time_step'] += 0.1*exp_time_step\n",
    "        exp_time_step += 1\n",
    "    else:\n",
    "        weights['time_step'] += 0\n",
    "        exp_time_step = 1\n",
    "\n",
    "    if energy_error > energy_error_fiducial:\n",
    "        weights['energy_loss'] += 0.1*exp_energy_loss\n",
    "        exp_energy_loss += 1\n",
    "    else:\n",
    "        weights['energy_loss'] += 0\n",
    "        exp_energy_loss = 1\n",
    "        \"\"\"\n",
    "\n",
    "    #print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {train_loss:.4e}, Test Loss: {test_loss:.4e}, Energy Loss: {energy_error:.4e}/{energy_error_fiducial:.4e}, std: {energy_error_std:.4e}/{energy_error_fiducial_std:.4e}, Time step: {time_step:.4e}/{time_step_fiducial:.4e}\")\n",
    "    #print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {train_res['train_loss']:.4e}, Val Loss: {val_res['val_loss']:.4e}, Energy Loss: {np.log10(train_res['energy_error']):.4e}/{np.log10(train_res['energy_error_fiducial']):.4e}, {np.log10(val_res['energy_error']):.4e}/{np.log10(val_res['energy_error_fiducial']):.4e}, Time step: {train_res['time_step']:.4e}/{train_res['time_step_fiducial']:.4e}, {val_res['time_step']:.4e}/{val_res['time_step_fiducial']:.4e}, {val_res['time_step_std']:.4e}/{val_res['time_step_fiducial_std']:.4e}\")\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Timestep error: {train_res['time_step_relative_error']:.4e}/{val_res['time_step_relative_error']:.4e}, Energy Loss: {np.log10(train_res['energy_error']):.4e}/{np.log10(train_res['energy_error_fiducial']):.4e}, {np.log10(val_res['energy_error']):.4e}/{np.log10(val_res['energy_error_fiducial']):.4e}, Time step: {train_res['time_step']:.4e}/{train_res['time_step_fiducial']:.4e}, {val_res['time_step']:.4e}/{val_res['time_step_fiducial']:.4e}, {val_res['time_step_std']:.4e}/{val_res['time_step_fiducial_std']:.4e}\")\n",
    "    #print(f\"Epoch [{epoch}/{num_epochs}], Train Loss: {train_res['train_loss']:.4e}, Val Loss: {val_res['val_loss']:.4e}, Energy Loss: {np.log10(train_res['energy_error']):.4e}/{np.log10(train_res['energy_error_fiducial']):.4e}, {np.log10(val_res['energy_error']):.4e}/{np.log10(val_res['energy_error_fiducial']):.4e}, energy_pred: {energy_pred:.4e}/{energy_init:.4e}, Time step: {time_step:.4e}/{time_step_fiducial:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
