---
phase: 02-history-buffer-zero-padding
plan: 02
type: execute
wave: 2
depends_on:
  - 02-01
files_modified:
  - src/history_buffer.py
autonomous: true

must_haves:
  truths:
    - "features_for_batch() returns zero-padded features for incomplete history"
    - "features_for_histories() returns zero-padded features per-buffer for incomplete histories"
    - "All three feature methods use consistent zero-padding strategy"
  artifacts:
    - path: "src/history_buffer.py"
      provides: "Zero-padding in features_for_batch()"
      pattern: "_zero_state.*features_for_batch"
    - path: "src/history_buffer.py"
      provides: "Zero-padding in features_for_histories()"
      pattern: "_zero_state.*features_for_histories"
  key_links:
    - from: "_zero_state()"
      to: "features_for_batch()"
      via: "called when len(past_list) < history_len"
      pattern: "_zero_state\\("
    - from: "_zero_state()"
      to: "features_for_histories()"
      via: "called when len(past_list) < history_len"
      pattern: "HistoryBuffer._zero_state\\("
---

<objective>
Implement zero-padding in features_for_batch() and features_for_histories()

Purpose: Complete the zero-padding implementation across all feature extraction methods. After Plan 01 added _zero_state() and updated features_for(), this plan updates the two remaining batch methods to use the same zero-padding strategy.

Output: Modified src/history_buffer.py with zero-padding in all three feature extraction methods.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-history-buffer-zero-padding/02-RESEARCH.md
@.planning/phases/02-history-buffer-zero-padding/02-01-SUMMARY.md
@src/history_buffer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Modify features_for_batch() to use zero-padding</name>
  <files>src/history_buffer.py</files>
  <action>
Modify the `features_for_batch()` method (lines 294-348) to use `_zero_state()` for padding instead of repeating the oldest state.

Current code (lines 309-316):
```python
past_list: List[_HistoryState] = list(self._buf)
if len(past_list) < self.history_len:
    pad_count = self.history_len - len(past_list)
    if past_list:
        past_list = [past_list[0]] * pad_count + past_list
    else:
        past_list = []
```

Replace with:
```python
past_list: List[_HistoryState] = list(self._buf)
if len(past_list) < self.history_len:
    pad_count = self.history_len - len(past_list)
    if past_list:
        # Use oldest state as reference for shape/device/dtype
        zero_state = self._zero_state(past_list[0])
    else:
        # Use current_state as reference (already constructed above)
        zero_state = self._zero_state(current_state)

    past_list = [zero_state] * pad_count + past_list
```

IMPORTANT: Note that `current_state` is constructed at line 317-324 (after the padding logic). You need to move the `current_state` construction BEFORE the padding logic so it can be used as a reference when `past_list` is empty.

Reorder the code to:
1. Construct `current_state` from batch_state tensors
2. Padding logic (using `current_state` as reference when past_list is empty)
3. Build seq and continue

The rest of the method remains unchanged.
  </action>
  <verify>
Run Python to verify batch zero-padding behavior:
```bash
cd /u/gkerex/projects/AITimeStepper && python -c "
from src.history_buffer import HistoryBuffer
from src.particle import ParticleTorch
import torch

# Create history buffer
hb = HistoryBuffer(history_len=3, feature_type='basic')

# Create batched particle (B=2)
p = ParticleTorch(
    position=torch.randn(2, 4, 3),
    velocity=torch.randn(2, 4, 3),
    mass=torch.ones(4),
    dt=torch.tensor([0.01, 0.01]),
    softening=0.1
)

# Get features with empty history
feats = hb.features_for_batch(p)
print(f'Batch feature shape: {feats.shape}')
assert feats.shape[0] == 2, f'Expected batch size 2, got {feats.shape[0]}'
assert feats.shape[1] == 44, f'Expected 44 features, got {feats.shape[1]}'
print('features_for_batch() zero-padding works correctly')
"
```
  </verify>
  <done>features_for_batch() uses zero-padding when history is incomplete; returns correct shape for batched input</done>
</task>

<task type="auto">
  <name>Task 2: Modify features_for_histories() to use zero-padding</name>
  <files>src/history_buffer.py</files>
  <action>
Modify the `features_for_histories()` static method (lines 350-434) to use `_zero_state()` for padding instead of repeating states.

Current code (lines 401-406, inside the per-history loop):
```python
if len(past_list) < history_len:
    pad_count = history_len - len(past_list)
    if past_list:
        past_list = [past_list[0]] * pad_count + past_list
    else:
        past_list = [current_state] * pad_count
```

Replace with:
```python
if len(past_list) < history_len:
    pad_count = history_len - len(past_list)
    if past_list:
        # Use oldest state as reference
        zero_state = HistoryBuffer._zero_state(past_list[0])
    else:
        # Use current_state as reference
        zero_state = HistoryBuffer._zero_state(current_state)

    past_list = [zero_state] * pad_count + past_list
```

Note: This is inside a `for i, history in enumerate(histories):` loop, and `current_state` is already constructed before this padding logic (lines 392-399). The method is static, so use `HistoryBuffer._zero_state()` instead of `self._zero_state()`.

The rest of the method remains unchanged.
  </action>
  <verify>
Run Python to verify per-history zero-padding:
```bash
cd /u/gkerex/projects/AITimeStepper && python -c "
from src.history_buffer import HistoryBuffer
from src.particle import ParticleTorch
import torch

# Create multiple history buffers (one per batch element)
hb1 = HistoryBuffer(history_len=3, feature_type='basic')
hb2 = HistoryBuffer(history_len=3, feature_type='basic')

# Create batched particle (B=2)
p = ParticleTorch(
    position=torch.randn(2, 4, 3),
    velocity=torch.randn(2, 4, 3),
    mass=torch.ones(4),
    dt=torch.tensor([0.01, 0.01]),
    softening=0.1
)

# Get features using per-history method
feats = HistoryBuffer.features_for_histories([hb1, hb2], p)
print(f'Multi-history feature shape: {feats.shape}')
assert feats.shape[0] == 2, f'Expected batch size 2, got {feats.shape[0]}'
assert feats.shape[1] == 44, f'Expected 44 features, got {feats.shape[1]}'
print('features_for_histories() zero-padding works correctly')
"
```
  </verify>
  <done>features_for_histories() uses zero-padding when individual histories are incomplete</done>
</task>

<task type="auto">
  <name>Task 3: Extend test function for batch methods</name>
  <files>src/history_buffer.py</files>
  <action>
Update the `_test_zero_padding()` function (added in Plan 01) to include tests for the batch methods.

Add the following test cases to the existing function, before the final print statement:

```python
    # Test 5: features_for_batch with empty buffer
    hb_batch = HistoryBuffer(history_len=3, feature_type='basic')
    p_batch = ParticleTorch(
        position=torch.randn(2, 4, 3),
        velocity=torch.randn(2, 4, 3),
        mass=torch.ones(4),
        dt=torch.tensor([0.01, 0.01]),
        softening=0.1
    )
    feats_batch = hb_batch.features_for_batch(p_batch)
    assert feats_batch.shape == (2, 44), f"Expected (2, 44), got {feats_batch.shape}"

    # Test 6: features_for_histories with mixed buffer states
    hb_a = HistoryBuffer(history_len=3, feature_type='basic')
    hb_b = HistoryBuffer(history_len=3, feature_type='basic')
    # Push one state to hb_a, leave hb_b empty
    p_single = ParticleTorch(
        position=torch.randn(4, 3),
        velocity=torch.randn(4, 3),
        mass=torch.ones(4),
        dt=0.01,
        softening=0.1
    )
    hb_a.push(p_single)

    feats_multi = HistoryBuffer.features_for_histories([hb_a, hb_b], p_batch)
    assert feats_multi.shape == (2, 44), f"Expected (2, 44), got {feats_multi.shape}"

    # Test 7: delta_mag feature type with zero-padding
    hb_delta = HistoryBuffer(history_len=3, feature_type='delta_mag')
    feats_delta = hb_delta.features_for(p_single)
    # delta_mag: 10 features per transition, 3 transitions for history_len=3
    assert feats_delta.shape[-1] == 30, f"Expected 30 delta_mag features, got {feats_delta.shape[-1]}"
```

These tests verify:
- features_for_batch() works with empty buffer
- features_for_histories() works with mixed buffer states (one has data, one empty)
- delta_mag feature type works with zero-padding
  </action>
  <verify>
Run the extended test function:
```bash
cd /u/gkerex/projects/AITimeStepper && python -c "from src.history_buffer import _test_zero_padding; _test_zero_padding()"
```
  </verify>
  <done>Extended test function passes all assertions including batch method and delta_mag tests</done>
</task>

</tasks>

<verification>
Run all verification commands:
```bash
cd /u/gkerex/projects/AITimeStepper
python -c "from src.history_buffer import HistoryBuffer, _HistoryState; print('Import OK')"
python -c "from src.history_buffer import _test_zero_padding; _test_zero_padding()"

# Quick smoke test with runner
python run/runner.py simulate --num-particles 3 --steps 10 2>&1 | head -5
```

All should complete without errors.
</verification>

<success_criteria>
1. `features_for_batch()` uses `_zero_state()` for padding
2. `features_for_histories()` uses `HistoryBuffer._zero_state()` for padding
3. Extended `_test_zero_padding()` passes all assertions
4. All three feature methods now use consistent zero-padding strategy
5. Existing simulation functionality unaffected (smoke test passes)
</success_criteria>

<output>
After completion, create `.planning/phases/02-history-buffer-zero-padding/02-02-SUMMARY.md`
</output>
