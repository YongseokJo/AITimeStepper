---
phase: 05-unified-epoch-structure
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/unified_training.py
  - src/__init__.py
autonomous: true

must_haves:
  truths:
    - "run_two_phase_training() calls train_epoch_two_phase() for config.epochs iterations"
    - "Checkpoint saved every 10 epochs and on final epoch"
    - "W&B logging tracks Part 1 and Part 2 metrics per epoch"
    - "Function returns aggregated training results"
    - "History buffer persists across epochs (same instance)"
  artifacts:
    - path: "src/unified_training.py"
      provides: "run_two_phase_training function"
      min_lines: 180
      exports: ["train_epoch_two_phase", "run_two_phase_training"]
    - path: "src/__init__.py"
      provides: "Module exports"
      contains: "run_two_phase_training"
  key_links:
    - from: "src/unified_training.py"
      to: "src/checkpoint.py"
      via: "import save_checkpoint"
      pattern: "from .checkpoint import save_checkpoint"
    - from: "src/unified_training.py"
      to: "train_epoch_two_phase"
      via: "function call in loop"
      pattern: "train_epoch_two_phase\\("
---

<objective>
Implement `run_two_phase_training()` - the outer training loop that runs for N epochs with checkpointing and W&B logging.

Purpose: This function provides the complete training orchestration, calling `train_epoch_two_phase()` for each epoch, saving checkpoints at intervals, and logging metrics to W&B.

Output: Extended `src/unified_training.py` with `run_two_phase_training()` function.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-unified-epoch-structure/05-RESEARCH.md
@.planning/phases/05-unified-epoch-structure/05-01-SUMMARY.md

# Key source files
@src/unified_training.py
@src/checkpoint.py
@run/runner.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add run_two_phase_training() to unified_training.py</name>
  <files>src/unified_training.py</files>
  <action>
Extend `src/unified_training.py` with the `run_two_phase_training()` function.

Add the following import at the top of the file:
```python
from pathlib import Path
from .checkpoint import save_checkpoint
```

Add the following function after `train_epoch_two_phase()`:

```python
def run_two_phase_training(
    model: torch.nn.Module,
    particle: ParticleTorch,
    optimizer: torch.optim.Optimizer,
    config: Config,
    adapter: ModelAdapter,
    history_buffer: Optional[HistoryBuffer] = None,
    save_dir: Optional[Path] = None,
    wandb_run: Optional[Any] = None,
    checkpoint_interval: int = 10,
) -> Dict[str, Any]:
    """
    Run N epochs of two-phase training with checkpointing and logging.

    Implements TRAIN-09: Run for fixed N epochs (config.epochs).

    Training loop:
    1. For each epoch in range(config.epochs):
       a. Call train_epoch_two_phase() to execute Part 1 + Part 2
       b. Log metrics to W&B (if enabled)
       c. Save checkpoint every checkpoint_interval epochs and on final epoch
    2. Return aggregated training results

    IMPORTANT: The same history_buffer instance is passed to all epochs.
    This preserves temporal context across epochs (per 05-RESEARCH.md).

    IMPORTANT: The particle state is NOT reset between epochs by default.
    Each epoch continues from the final state of the previous epoch.
    For fresh ICs each epoch, caller should handle particle reset.

    Args:
        model: Neural network for dt prediction
        particle: Initial particle state
        optimizer: PyTorch optimizer for model
        config: Config with epochs, energy_threshold, steps_per_epoch, etc.
        adapter: ModelAdapter for feature construction
        history_buffer: Optional history buffer (persists across epochs)
        save_dir: Directory for checkpoint saves (None = no saves)
        wandb_run: W&B run object from wandb.init() (None = no logging)
        checkpoint_interval: Save checkpoint every N epochs (default: 10)

    Returns:
        training_result: dict with:
            'epochs_completed': int - Number of epochs run
            'total_time': float - Total training time (seconds)
            'final_metrics': dict - Metrics from last epoch
            'convergence_rate': float - Fraction of epochs where Part 2 converged
            'results': List[dict] - Per-epoch results (if config.debug)

    Example:
        >>> result = run_two_phase_training(
        ...     model, particle, optimizer, config, adapter,
        ...     history_buffer=history_buffer,
        ...     save_dir=Path("data/my_run/model"),
        ...     wandb_run=wandb_run,
        ... )
        >>> print(f"Trained for {result['epochs_completed']} epochs")
        >>> print(f"Convergence rate: {result['convergence_rate']:.1%}")
    """
    # Import wandb only if needed (avoid import error if not installed)
    wandb = None
    if wandb_run is not None:
        try:
            import wandb as wandb_lib
            wandb = wandb_lib
        except ImportError:
            warnings.warn("wandb_run provided but wandb not installed; logging disabled")
            wandb_run = None

    # Track overall training
    training_start = time.perf_counter()
    all_results: List[Dict[str, Any]] = []
    converged_count = 0

    # Current particle (will be updated each epoch if continuing trajectory)
    current_particle = particle

    for epoch in range(config.epochs):
        # Execute one epoch (Part 1 + Part 2)
        epoch_result = train_epoch_two_phase(
            model=model,
            particle=current_particle,
            optimizer=optimizer,
            config=config,
            adapter=adapter,
            history_buffer=history_buffer,
        )

        # Track convergence
        if epoch_result['converged']:
            converged_count += 1

        # Store results (only in debug mode to save memory)
        if config.debug:
            all_results.append(epoch_result)

        # W&B logging
        if wandb_run is not None and wandb is not None:
            traj_m = epoch_result['trajectory_metrics']
            gen_m = epoch_result['generalization_metrics']

            # Compute acceptance rate: 1 / (1 + mean_retrain_iterations)
            mean_retrain = traj_m.get('mean_retrain_iterations', 0.0)
            acceptance_rate = 1.0 / (1.0 + mean_retrain) if mean_retrain >= 0 else 0.0

            log_data = {
                'epoch': epoch,
                'epoch_time': epoch_result['epoch_time'],
                # Part 1 metrics
                'part1/trajectory_length': traj_m.get('trajectory_length', 0),
                'part1/mean_retrain_iterations': mean_retrain,
                'part1/max_retrain_iterations': traj_m.get('max_retrain_iterations', 0),
                'part1/mean_energy_error': traj_m.get('mean_energy_error', 0.0),
                'part1/warmup_discarded': traj_m.get('warmup_discarded', 0),
                'part1/acceptance_rate': acceptance_rate,
                # Part 2 metrics
                'part2/converged': int(epoch_result['converged']),
                'part2/iterations': epoch_result['part2_iterations'],
                'part2/final_pass_rate': gen_m.get('final_pass_rate', 0.0),
                'part2/mean_rel_dE': gen_m.get('mean_rel_dE', 0.0),
                'part2/max_rel_dE': gen_m.get('max_rel_dE', 0.0),
            }
            wandb.log(log_data)

        # Checkpointing: every checkpoint_interval epochs and on final epoch
        if save_dir is not None:
            is_checkpoint_epoch = (epoch % checkpoint_interval == 0) or (epoch == config.epochs - 1)
            if is_checkpoint_epoch:
                save_path = Path(save_dir) / f"model_epoch_{epoch:04d}.pt"
                save_checkpoint(
                    save_path,
                    model,
                    optimizer,
                    epoch=epoch,
                    logs=epoch_result,
                    config=config,
                )
                if config.debug:
                    print(f"Checkpoint saved: {save_path}")

        # Print progress (every 10 epochs or in debug mode)
        if config.debug or (epoch % 10 == 0) or (epoch == config.epochs - 1):
            traj_len = epoch_result['trajectory_metrics']['trajectory_length']
            conv_str = "converged" if epoch_result['converged'] else f"max_iter({epoch_result['part2_iterations']})"
            print(f"Epoch {epoch}: traj_len={traj_len}, part2={conv_str}, time={epoch_result['epoch_time']:.2f}s")

    # Compute final results
    total_time = time.perf_counter() - training_start
    convergence_rate = converged_count / config.epochs if config.epochs > 0 else 0.0

    return {
        'epochs_completed': config.epochs,
        'total_time': total_time,
        'final_metrics': epoch_result if config.epochs > 0 else {},
        'convergence_rate': convergence_rate,
        'results': all_results if config.debug else [],
    }
```

Key implementation notes:
- Pass same `history_buffer` instance to all epochs (per 05-RESEARCH.md recommendation)
- Checkpoint every `checkpoint_interval` epochs (default 10) AND on final epoch
- W&B logs: Part 1 metrics (trajectory_length, acceptance_rate, mean_retrain_iterations) and Part 2 metrics (converged, iterations, pass_rate)
- Acceptance rate derived as `1 / (1 + mean_retrain_iterations)` per research doc
- Progress printed every 10 epochs for user feedback
- Results list only populated in debug mode to save memory
- Uses `save_checkpoint` from `src/checkpoint.py` (established pattern)
  </action>
  <verify>
Run: `python -c "from src.unified_training import run_two_phase_training; print('Import OK')"`
Expected: "Import OK" printed without errors
  </verify>
  <done>
- `run_two_phase_training()` function exists with correct signature
- Function calls `train_epoch_two_phase()` in a loop for config.epochs iterations
- Checkpoint saved every checkpoint_interval epochs and on final epoch
- W&B logging includes Part 1 and Part 2 metrics
- Returns training results dict with epochs_completed, total_time, convergence_rate
  </done>
</task>

<task type="auto">
  <name>Task 2: Export run_two_phase_training from src/__init__.py</name>
  <files>src/__init__.py</files>
  <action>
Update the unified_training import in `src/__init__.py` to include `run_two_phase_training`:

Change:
```python
from .unified_training import (
    train_epoch_two_phase,
)
```

To:
```python
from .unified_training import (
    train_epoch_two_phase,
    run_two_phase_training,
)
```

This makes both functions available as `from src import train_epoch_two_phase, run_two_phase_training`.
  </action>
  <verify>
Run: `python -c "from src import run_two_phase_training; print('Export OK')"`
Expected: "Export OK" printed without errors
  </verify>
  <done>
- `run_two_phase_training` exported from `src/__init__.py`
- Function importable via `from src import run_two_phase_training`
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify checkpointing and multi-epoch behavior</name>
  <files>None (verification only)</files>
  <action>
Run integration test to verify checkpointing and multi-epoch training:

```python
import tempfile
from pathlib import Path
import torch
from src import Config, ParticleTorch, ModelAdapter, run_two_phase_training

# Create test setup
config = Config(
    epochs=5,
    energy_threshold=0.1,
    steps_per_epoch=3,
    replay_steps=5,
    replay_batch_size=2,
    min_replay_size=1,
    debug=True,  # Enable results tracking
)

# Simple 2-body particle
pos = torch.tensor([[0.5, 0.0], [-0.5, 0.0]], dtype=torch.float64)
vel = torch.tensor([[0.0, 1.0], [0.0, -1.0]], dtype=torch.float64)
mass = torch.tensor([1.0, 1.0], dtype=torch.float64)
particle = ParticleTorch.from_tensors(mass=mass, position=pos, velocity=vel, dt=0.001)

adapter = ModelAdapter(config)
input_dim = adapter.input_dim_from_state(particle)

model = torch.nn.Sequential(
    torch.nn.Linear(input_dim, 2, dtype=torch.float64),
)
torch.nn.init.constant_(model[0].weight, 0.0)
torch.nn.init.constant_(model[0].bias, 0.001)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Run with temp directory for checkpoints
with tempfile.TemporaryDirectory() as tmpdir:
    save_dir = Path(tmpdir)

    result = run_two_phase_training(
        model, particle, optimizer, config, adapter,
        save_dir=save_dir,
        checkpoint_interval=2,  # Save every 2 epochs
    )

    # Verify result structure
    assert result['epochs_completed'] == 5
    assert 'total_time' in result
    assert 'convergence_rate' in result
    assert 0.0 <= result['convergence_rate'] <= 1.0

    # Verify checkpoints created (epochs 0, 2, 4)
    checkpoints = list(save_dir.glob("model_epoch_*.pt"))
    assert len(checkpoints) >= 2, f"Expected at least 2 checkpoints, got {len(checkpoints)}"

    # Verify checkpoint content
    ckpt = torch.load(checkpoints[0])
    assert 'model_state_dict' in ckpt
    assert 'epoch' in ckpt
    assert 'config' in ckpt

print("Checkpoint integration test PASSED")
```
  </action>
  <verify>
Run the integration test script above.
Expected output: "Checkpoint integration test PASSED"
  </verify>
  <done>
- `run_two_phase_training()` runs for specified number of epochs
- Checkpoints saved at correct intervals
- Checkpoint files contain model_state_dict, epoch, config
- Returns correct result structure with epochs_completed, total_time, convergence_rate
  </done>
</task>

</tasks>

<verification>
1. `src/unified_training.py` has >= 180 lines
2. `run_two_phase_training()` function has correct signature
3. Function loops for config.epochs iterations
4. Checkpoints saved every checkpoint_interval epochs and on final epoch
5. W&B logging code present (conditional on wandb_run)
6. Function exported from `src/__init__.py`
7. Integration test passes with checkpoint verification
</verification>

<success_criteria>
- run_two_phase_training() runs for config.epochs iterations
- Checkpoint saved every 10 epochs (default) and on final epoch
- W&B logging tracks Part 1 acceptance rate and Part 2 iterations
- Same history_buffer instance persists across all epochs
- Returns aggregated results with convergence_rate and total_time
- Progress printed every 10 epochs for user feedback
</success_criteria>

<output>
After completion, create `.planning/phases/05-unified-epoch-structure/05-02-SUMMARY.md`
</output>
