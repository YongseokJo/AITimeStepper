---
phase: 05-unified-epoch-structure
plan: 03
type: execute
wave: 2
depends_on: ["05-01", "05-02"]
files_modified:
  - tests/test_unified_training.py
autonomous: true

must_haves:
  truths:
    - "Tests verify train_epoch_two_phase() calls Part 1 then Part 2"
    - "Tests verify trajectory passed from Part 1 to Part 2"
    - "Tests verify run_two_phase_training() runs for config.epochs"
    - "Tests verify checkpoint creation at intervals"
    - "Tests verify empty trajectory edge case handling"
    - "All tests pass in under 30 seconds"
  artifacts:
    - path: "tests/test_unified_training.py"
      provides: "Unit tests for unified training"
      min_lines: 300
      contains: "class TestTrainEpochTwoPhase"
  key_links:
    - from: "tests/test_unified_training.py"
      to: "src/unified_training.py"
      via: "import test targets"
      pattern: "from src.unified_training import"
    - from: "tests/test_unified_training.py"
      to: "pytest"
      via: "test framework"
      pattern: "import pytest"
---

<objective>
Create comprehensive unit tests for the unified epoch structure functions: `train_epoch_two_phase()` and `run_two_phase_training()`.

Purpose: Verify the epoch orchestration, trajectory handoff between parts, checkpointing behavior, and edge case handling.

Output: New test file `tests/test_unified_training.py` with >= 12 test cases.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-unified-epoch-structure/05-RESEARCH.md
@.planning/phases/05-unified-epoch-structure/05-01-SUMMARY.md
@.planning/phases/05-unified-epoch-structure/05-02-SUMMARY.md

# Test patterns from prior phases
@tests/test_trajectory_collection.py
@tests/test_generalization_training.py

# Target module
@src/unified_training.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test_unified_training.py with test fixtures</name>
  <files>tests/test_unified_training.py</files>
  <action>
Create `tests/test_unified_training.py` with shared test fixtures matching the patterns from Phase 3/4 tests:

```python
"""
Tests for unified epoch training functions.

Covers:
- train_epoch_two_phase (single epoch orchestration)
- run_two_phase_training (multi-epoch loop with checkpointing)

Test organization:
- TestTrainEpochTwoPhase: Tests for single epoch function
- TestRunTwoPhaseTraining: Tests for multi-epoch loop
"""

import tempfile
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest
import torch
import torch.nn as nn

from src.config import Config
from src.particle import ParticleTorch
from src.model_adapter import ModelAdapter
from src.history_buffer import HistoryBuffer
from src.unified_training import (
    train_epoch_two_phase,
    run_two_phase_training,
)


class MockModel(nn.Module):
    """Mock model that returns configurable dt values.

    Uses a linear layer with zero weights so output is always dt_value,
    but gradients still flow through the computation graph.
    Dynamically adjusts input dimension based on first forward pass.
    """

    def __init__(self, dt_value: float = 0.001, input_dim: int = 11):
        super().__init__()
        self.dt_value = dt_value
        self.input_dim = input_dim
        # Use float64 for compatibility with particle system
        self.linear = nn.Linear(input_dim, 2, dtype=torch.float64)
        # Zero out weights so output is just the bias
        nn.init.constant_(self.linear.weight, 0.0)
        nn.init.constant_(self.linear.bias, dt_value)

    def forward(self, x):
        x = x.to(dtype=self.linear.weight.dtype)
        if x.shape[-1] != self.linear.in_features:
            new_linear = nn.Linear(x.shape[-1], 2, dtype=torch.float64, device=x.device)
            nn.init.constant_(new_linear.weight, 0.0)
            nn.init.constant_(new_linear.bias, self.dt_value)
            self.linear = new_linear
        out = self.linear(x)
        return torch.abs(out) + 1e-9


class TrainableMockModel(nn.Module):
    """Mock model with trainable parameters for testing optimizer behavior."""

    def __init__(self, input_dim: int = 11, initial_dt: float = 0.1):
        super().__init__()
        self.initial_dt = initial_dt
        self.fc = nn.Linear(input_dim, 2, dtype=torch.float64)
        nn.init.constant_(self.fc.weight, 0.0)
        nn.init.constant_(self.fc.bias, initial_dt)

    def forward(self, x):
        x = x.to(dtype=self.fc.weight.dtype)
        if x.shape[-1] != self.fc.in_features:
            new_fc = nn.Linear(x.shape[-1], 2, dtype=torch.float64, device=x.device)
            nn.init.constant_(new_fc.weight, 0.0)
            nn.init.constant_(new_fc.bias, self.initial_dt)
            self.fc = new_fc
        return torch.abs(self.fc(x)) + 1e-6


@pytest.fixture
def simple_particle():
    """Create a simple 2-body particle system."""
    pos = torch.tensor([[0.5, 0.0], [-0.5, 0.0]], dtype=torch.float64)
    vel = torch.tensor([[0.0, 1.0], [0.0, -1.0]], dtype=torch.float64)
    mass = torch.tensor([1.0, 1.0], dtype=torch.float64)
    return ParticleTorch.from_tensors(mass=mass, position=pos, velocity=vel, dt=0.001)


@pytest.fixture
def config():
    """Create a test configuration with relaxed thresholds for fast testing."""
    return Config(
        energy_threshold=0.1,  # 10% for fast testing
        steps_per_epoch=3,
        replay_steps=10,
        replay_batch_size=2,
        min_replay_size=1,
        epochs=5,
        debug=False,
    )


@pytest.fixture
def adapter(config):
    """Create a ModelAdapter for testing."""
    return ModelAdapter(config)


# Tests continue below...
```

This establishes the test infrastructure. The MockModel and TrainableMockModel classes match the patterns from test_trajectory_collection.py and test_generalization_training.py.
  </action>
  <verify>
Run: `python -c "from tests.test_unified_training import MockModel, TrainableMockModel; print('Fixtures OK')"`
Expected: "Fixtures OK" printed without errors
  </verify>
  <done>
- Test file created with MockModel and TrainableMockModel classes
- Fixtures for simple_particle, config, adapter defined
- Import structure matches existing test patterns
  </done>
</task>

<task type="auto">
  <name>Task 2: Add TestTrainEpochTwoPhase test class</name>
  <files>tests/test_unified_training.py</files>
  <action>
Add the TestTrainEpochTwoPhase class to `tests/test_unified_training.py`:

```python
class TestTrainEpochTwoPhase:
    """Tests for train_epoch_two_phase() function."""

    def test_returns_expected_structure(self, simple_particle, config, adapter):
        """Verify return dict has all expected keys."""
        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        result = train_epoch_two_phase(
            model, simple_particle, optimizer, config, adapter
        )

        # Check all required keys present
        assert 'trajectory_metrics' in result
        assert 'generalization_metrics' in result
        assert 'converged' in result
        assert 'part2_iterations' in result
        assert 'epoch_time' in result

        # Check types
        assert isinstance(result['trajectory_metrics'], dict)
        assert isinstance(result['generalization_metrics'], dict)
        assert isinstance(result['converged'], bool)
        assert isinstance(result['part2_iterations'], int)
        assert isinstance(result['epoch_time'], float)

    def test_trajectory_metrics_structure(self, simple_particle, config, adapter):
        """Verify trajectory_metrics contains Part 1 output."""
        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        result = train_epoch_two_phase(
            model, simple_particle, optimizer, config, adapter
        )

        traj_m = result['trajectory_metrics']
        assert 'total_steps' in traj_m
        assert 'warmup_discarded' in traj_m
        assert 'trajectory_length' in traj_m
        assert 'mean_retrain_iterations' in traj_m
        assert 'mean_energy_error' in traj_m

        # trajectory_length should match steps_per_epoch (no history buffer)
        assert traj_m['trajectory_length'] == config.steps_per_epoch

    def test_generalization_metrics_structure(self, simple_particle, config, adapter):
        """Verify generalization_metrics contains Part 2 output."""
        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        result = train_epoch_two_phase(
            model, simple_particle, optimizer, config, adapter
        )

        gen_m = result['generalization_metrics']
        assert 'mean_rel_dE' in gen_m
        assert 'max_rel_dE' in gen_m
        assert 'final_pass_rate' in gen_m

    def test_part1_output_fed_to_part2(self, simple_particle, config, adapter):
        """Verify trajectory from Part 1 is processed by Part 2."""
        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        result = train_epoch_two_phase(
            model, simple_particle, optimizer, config, adapter
        )

        # If trajectory was collected, Part 2 should have run
        traj_len = result['trajectory_metrics']['trajectory_length']
        if traj_len > 0:
            # Part 2 should have done at least some work
            assert result['part2_iterations'] >= 0

    def test_epoch_time_measured(self, simple_particle, config, adapter):
        """Verify epoch_time is positive and reasonable."""
        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        result = train_epoch_two_phase(
            model, simple_particle, optimizer, config, adapter
        )

        assert result['epoch_time'] > 0
        assert result['epoch_time'] < 60  # Should not take > 60 seconds

    def test_with_history_buffer(self, simple_particle, adapter):
        """Verify function works with history buffer enabled."""
        config = Config(
            energy_threshold=0.1,
            steps_per_epoch=5,
            history_len=2,
            replay_steps=5,
            replay_batch_size=2,
            min_replay_size=1,
        )
        adapter = ModelAdapter(config)

        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        history_buffer = HistoryBuffer(
            history_len=config.history_len,
            feature_type=config.feature_type
        )
        # Pre-populate to avoid zero-padding NaN issues
        history_buffer.push(simple_particle.clone_detached())
        history_buffer.push(simple_particle.clone_detached())

        result = train_epoch_two_phase(
            model, simple_particle, optimizer, config, adapter,
            history_buffer=history_buffer
        )

        # warmup should be discarded
        traj_m = result['trajectory_metrics']
        assert traj_m['warmup_discarded'] == config.history_len
        # trajectory_length should be steps_per_epoch - history_len
        expected_len = config.steps_per_epoch - config.history_len
        assert traj_m['trajectory_length'] == expected_len

    def test_empty_trajectory_edge_case(self, simple_particle, adapter):
        """Verify function handles empty trajectory (all warmup)."""
        config = Config(
            energy_threshold=0.1,
            steps_per_epoch=2,  # <= history_len
            history_len=3,
            replay_steps=5,
            replay_batch_size=2,
            min_replay_size=1,
        )
        adapter = ModelAdapter(config)

        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        history_buffer = HistoryBuffer(
            history_len=config.history_len,
            feature_type=config.feature_type
        )
        # Pre-populate
        for _ in range(config.history_len):
            history_buffer.push(simple_particle.clone_detached())

        # Should issue warning but not crash
        with pytest.warns(UserWarning, match="Empty trajectory"):
            result = train_epoch_two_phase(
                model, simple_particle, optimizer, config, adapter,
                history_buffer=history_buffer
            )

        # Part 2 should return immediately for empty trajectory
        assert result['trajectory_metrics']['trajectory_length'] == 0
        assert result['converged'] == True  # Empty trajectory = trivially converged
        assert result['part2_iterations'] == 0
```
  </action>
  <verify>
Run: `python -m pytest tests/test_unified_training.py::TestTrainEpochTwoPhase -v --tb=short`
Expected: All tests pass
  </verify>
  <done>
- TestTrainEpochTwoPhase class added with 8 test methods
- Tests cover return structure, Part 1 metrics, Part 2 metrics, history buffer, edge cases
- Empty trajectory edge case verified with warning capture
  </done>
</task>

<task type="auto">
  <name>Task 3: Add TestRunTwoPhaseTraining test class</name>
  <files>tests/test_unified_training.py</files>
  <action>
Add the TestRunTwoPhaseTraining class to `tests/test_unified_training.py`:

```python
class TestRunTwoPhaseTraining:
    """Tests for run_two_phase_training() function."""

    def test_runs_for_config_epochs(self, simple_particle, adapter):
        """Verify function runs for exactly config.epochs iterations."""
        config = Config(
            epochs=3,
            energy_threshold=0.1,
            steps_per_epoch=2,
            replay_steps=5,
            replay_batch_size=2,
            min_replay_size=1,
        )
        adapter = ModelAdapter(config)

        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        result = run_two_phase_training(
            model, simple_particle, optimizer, config, adapter
        )

        assert result['epochs_completed'] == 3

    def test_returns_expected_structure(self, simple_particle, config, adapter):
        """Verify return dict has all expected keys."""
        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        result = run_two_phase_training(
            model, simple_particle, optimizer, config, adapter
        )

        assert 'epochs_completed' in result
        assert 'total_time' in result
        assert 'final_metrics' in result
        assert 'convergence_rate' in result
        assert 'results' in result

    def test_checkpoint_creation(self, simple_particle, adapter):
        """Verify checkpoints created at correct intervals."""
        config = Config(
            epochs=5,
            energy_threshold=0.1,
            steps_per_epoch=2,
            replay_steps=3,
            replay_batch_size=2,
            min_replay_size=1,
        )
        adapter = ModelAdapter(config)

        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        with tempfile.TemporaryDirectory() as tmpdir:
            save_dir = Path(tmpdir)

            result = run_two_phase_training(
                model, simple_particle, optimizer, config, adapter,
                save_dir=save_dir,
                checkpoint_interval=2,
            )

            # Should have checkpoints at epochs 0, 2, 4 (interval=2 + final)
            checkpoints = sorted(save_dir.glob("model_epoch_*.pt"))
            assert len(checkpoints) >= 2

            # Verify checkpoint naming
            checkpoint_epochs = [int(p.stem.split('_')[-1]) for p in checkpoints]
            assert 0 in checkpoint_epochs  # First epoch
            assert 4 in checkpoint_epochs  # Final epoch (5-1=4)

    def test_checkpoint_content(self, simple_particle, config, adapter):
        """Verify checkpoint files contain required data."""
        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        with tempfile.TemporaryDirectory() as tmpdir:
            save_dir = Path(tmpdir)

            run_two_phase_training(
                model, simple_particle, optimizer, config, adapter,
                save_dir=save_dir,
                checkpoint_interval=10,  # Just first and last
            )

            checkpoints = list(save_dir.glob("model_epoch_*.pt"))
            assert len(checkpoints) >= 1

            ckpt = torch.load(checkpoints[0])
            assert 'model_state_dict' in ckpt
            assert 'optimizer_state_dict' in ckpt
            assert 'epoch' in ckpt
            assert 'config' in ckpt

    def test_convergence_rate_tracking(self, simple_particle, adapter):
        """Verify convergence_rate is computed correctly."""
        config = Config(
            epochs=4,
            energy_threshold=0.1,
            steps_per_epoch=2,
            replay_steps=50,  # More iterations for convergence
            replay_batch_size=2,
            min_replay_size=1,
        )
        adapter = ModelAdapter(config)

        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        result = run_two_phase_training(
            model, simple_particle, optimizer, config, adapter
        )

        # convergence_rate should be between 0 and 1
        assert 0.0 <= result['convergence_rate'] <= 1.0

    def test_total_time_measured(self, simple_particle, config, adapter):
        """Verify total_time is positive and reasonable."""
        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        result = run_two_phase_training(
            model, simple_particle, optimizer, config, adapter
        )

        assert result['total_time'] > 0
        assert result['total_time'] < 120  # Should not take > 2 minutes

    def test_history_buffer_persists_across_epochs(self, simple_particle, adapter):
        """Verify same history buffer instance used across epochs."""
        config = Config(
            epochs=3,
            energy_threshold=0.1,
            steps_per_epoch=3,
            history_len=2,
            replay_steps=5,
            replay_batch_size=2,
            min_replay_size=1,
        )
        adapter = ModelAdapter(config)

        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        history_buffer = HistoryBuffer(
            history_len=config.history_len,
            feature_type=config.feature_type
        )
        # Pre-populate
        history_buffer.push(simple_particle.clone_detached())
        history_buffer.push(simple_particle.clone_detached())

        initial_len = len(history_buffer)

        result = run_two_phase_training(
            model, simple_particle, optimizer, config, adapter,
            history_buffer=history_buffer,
        )

        # Buffer should have grown (states pushed during collection)
        # Each epoch pushes steps_per_epoch states
        assert len(history_buffer) >= initial_len

    def test_debug_mode_stores_results(self, simple_particle, adapter):
        """Verify results list populated in debug mode."""
        config = Config(
            epochs=3,
            energy_threshold=0.1,
            steps_per_epoch=2,
            replay_steps=3,
            replay_batch_size=2,
            min_replay_size=1,
            debug=True,
        )
        adapter = ModelAdapter(config)

        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        result = run_two_phase_training(
            model, simple_particle, optimizer, config, adapter
        )

        # In debug mode, results list should have one entry per epoch
        assert len(result['results']) == config.epochs

    def test_no_save_dir_skips_checkpointing(self, simple_particle, config, adapter):
        """Verify no error when save_dir is None."""
        model = MockModel(dt_value=0.001)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        # Should not raise any errors
        result = run_two_phase_training(
            model, simple_particle, optimizer, config, adapter,
            save_dir=None,
        )

        assert result['epochs_completed'] == config.epochs
```
  </action>
  <verify>
Run: `python -m pytest tests/test_unified_training.py::TestRunTwoPhaseTraining -v --tb=short`
Expected: All tests pass
  </verify>
  <done>
- TestRunTwoPhaseTraining class added with 10 test methods
- Tests cover epoch counting, checkpointing, convergence tracking, history buffer persistence
- Debug mode and edge cases verified
  </done>
</task>

<task type="auto">
  <name>Task 4: Run full test suite and verify timing</name>
  <files>None (verification only)</files>
  <action>
Run the complete test suite for unified_training.py and verify all tests pass within the time constraint:

```bash
python -m pytest tests/test_unified_training.py -v --tb=short --durations=5
```

Expected:
- All tests pass (18 tests total)
- Total time < 30 seconds
- No warnings except expected UserWarning for empty trajectory test
  </action>
  <verify>
Run: `python -m pytest tests/test_unified_training.py -v --tb=short`
Expected: All tests pass, total time < 30 seconds
  </verify>
  <done>
- All 18 tests pass
- Test suite completes in under 30 seconds
- No unexpected warnings or errors
  </done>
</task>

</tasks>

<verification>
1. `tests/test_unified_training.py` exists with >= 300 lines
2. TestTrainEpochTwoPhase class has >= 7 test methods
3. TestRunTwoPhaseTraining class has >= 9 test methods
4. All tests pass: `python -m pytest tests/test_unified_training.py -v`
5. Test suite completes in < 30 seconds
6. Tests cover: return structure, metrics, checkpointing, history buffer, edge cases
</verification>

<success_criteria>
- TestTrainEpochTwoPhase: 7-8 tests covering single epoch behavior
- TestRunTwoPhaseTraining: 9-10 tests covering multi-epoch loop
- Checkpoint creation and content verified
- History buffer persistence across epochs verified
- Empty trajectory edge case tested
- All tests pass in under 30 seconds
- Test file has >= 300 lines
</success_criteria>

<output>
After completion, create `.planning/phases/05-unified-epoch-structure/05-03-SUMMARY.md`
</output>
