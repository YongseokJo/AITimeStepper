---
phase: 05-unified-epoch-structure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/unified_training.py
  - src/__init__.py
autonomous: true

must_haves:
  truths:
    - "train_epoch_two_phase() calls collect_trajectory() then generalize_on_trajectory()"
    - "Part 1 output (trajectory) is passed directly to Part 2"
    - "Epoch result contains metrics from both parts"
    - "Function handles empty trajectory edge case"
  artifacts:
    - path: "src/unified_training.py"
      provides: "train_epoch_two_phase function"
      min_lines: 80
      exports: ["train_epoch_two_phase"]
    - path: "src/__init__.py"
      provides: "Module exports"
      contains: "train_epoch_two_phase"
  key_links:
    - from: "src/unified_training.py"
      to: "src/trajectory_collection.py"
      via: "import collect_trajectory"
      pattern: "from .trajectory_collection import collect_trajectory"
    - from: "src/unified_training.py"
      to: "src/generalization_training.py"
      via: "import generalize_on_trajectory"
      pattern: "from .generalization_training import generalize_on_trajectory"
---

<objective>
Implement `train_epoch_two_phase()` - the core orchestrator that executes one complete epoch by combining Part 1 (trajectory collection) and Part 2 (generalization training).

Purpose: This function is the heart of the two-phase training system. It calls the Phase 3 trajectory collector, passes the output to the Phase 4 generalizer, and returns combined metrics.

Output: New module `src/unified_training.py` with `train_epoch_two_phase()` function.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-unified-epoch-structure/05-RESEARCH.md

# Key source files
@src/trajectory_collection.py
@src/generalization_training.py
@src/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create unified_training.py with train_epoch_two_phase()</name>
  <files>src/unified_training.py</files>
  <action>
Create new module `src/unified_training.py` with the following implementation:

```python
"""
Unified epoch training for two-phase training system.

This module provides the epoch orchestrator that combines:
- Part 1: Trajectory collection (collect_trajectory)
- Part 2: Generalization training (generalize_on_trajectory)

One epoch = Part 1 + Part 2. The trajectory from Part 1 is passed
directly to Part 2 for generalization training.

Phase 5 of AITimeStepper training refactor.
Requirements: TRAIN-08, TRAIN-09
"""

import time
import warnings
from typing import Any, Dict, List, Optional, Tuple

import torch

from .config import Config
from .history_buffer import HistoryBuffer
from .model_adapter import ModelAdapter
from .particle import ParticleTorch
from .trajectory_collection import collect_trajectory
from .generalization_training import generalize_on_trajectory


def train_epoch_two_phase(
    model: torch.nn.Module,
    particle: ParticleTorch,
    optimizer: torch.optim.Optimizer,
    config: Config,
    adapter: ModelAdapter,
    history_buffer: Optional[HistoryBuffer] = None,
) -> Dict[str, Any]:
    """
    Execute one complete epoch: Part 1 (collection) + Part 2 (generalization).

    Implements TRAIN-08: One epoch = Part 1 + Part 2.

    The epoch proceeds as follows:
    1. Part 1: Collect validated trajectory using collect_trajectory()
       - Predicts dt, integrates, accepts/rejects based on energy threshold
       - Collects config.steps_per_epoch accepted samples
       - Discards warmup steps (first history_len)
    2. Part 2: Generalize on trajectory using generalize_on_trajectory()
       - Samples random minibatches from collected trajectory
       - Trains until all samples pass OR max iterations (config.replay_steps)

    The epoch completes after Part 2 finishes (either converged or max iterations).

    Args:
        model: Neural network for dt prediction
        particle: Starting particle state for this epoch
        optimizer: PyTorch optimizer for model
        config: Config with energy_threshold, steps_per_epoch, replay_steps, etc.
        adapter: ModelAdapter for feature construction
        history_buffer: Optional history buffer for temporal features

    Returns:
        epoch_result: dict with:
            'trajectory_metrics': dict from collect_trajectory()
                - 'total_steps', 'warmup_discarded', 'trajectory_length'
                - 'mean_retrain_iterations', 'mean_energy_error', 'max_retrain_iterations'
            'generalization_metrics': dict from generalize_on_trajectory()
                - 'mean_rel_dE', 'max_rel_dE', 'final_pass_rate'
            'converged': bool - True if Part 2 converged before max iterations
            'part2_iterations': int - Number of iterations in Part 2
            'epoch_time': float - Wall clock time for epoch (seconds)

    Example:
        >>> epoch_result = train_epoch_two_phase(
        ...     model, particle, optimizer, config, adapter, history_buffer
        ... )
        >>> print(f"Trajectory: {epoch_result['trajectory_metrics']['trajectory_length']} samples")
        >>> print(f"Converged: {epoch_result['converged']} in {epoch_result['part2_iterations']} iterations")
    """
    epoch_start = time.perf_counter()

    # Part 1: Collect trajectory (TRAIN-04, HIST-02)
    trajectory, traj_metrics = collect_trajectory(
        model=model,
        particle=particle,
        optimizer=optimizer,
        config=config,
        adapter=adapter,
        history_buffer=history_buffer,
    )

    # Log warning if trajectory is empty (edge case)
    if not trajectory:
        warnings.warn(
            f"train_epoch_two_phase: Empty trajectory collected "
            f"(steps_per_epoch={config.steps_per_epoch}, history_len={config.history_len})",
            UserWarning,
        )

    # Part 2: Generalize on trajectory (TRAIN-05, TRAIN-06, TRAIN-07)
    converged, part2_iters, gen_metrics = generalize_on_trajectory(
        model=model,
        trajectory=trajectory,
        optimizer=optimizer,
        config=config,
        adapter=adapter,
        history_buffer=history_buffer,
    )

    epoch_time = time.perf_counter() - epoch_start

    return {
        'trajectory_metrics': traj_metrics,
        'generalization_metrics': gen_metrics,
        'converged': converged,
        'part2_iterations': part2_iters,
        'epoch_time': epoch_time,
    }
```

Key implementation notes:
- Import both `collect_trajectory` and `generalize_on_trajectory` from their modules
- Pass the `trajectory` list directly from Part 1 to Part 2 (no transformation needed)
- Track wall clock time with `time.perf_counter()` for epoch timing metrics
- Issue warning for empty trajectory (edge case when steps_per_epoch <= history_len)
- Epoch completes after Part 2 finishes (TRAIN-08 requirement)
  </action>
  <verify>
Run: `python -c "from src.unified_training import train_epoch_two_phase; print('Import OK')"`
Expected: "Import OK" printed without errors
  </verify>
  <done>
- `train_epoch_two_phase()` function exists with correct signature
- Function calls `collect_trajectory()` then `generalize_on_trajectory()`
- Part 1 trajectory passed directly to Part 2
- Returns dict with trajectory_metrics, generalization_metrics, converged, part2_iterations, epoch_time
  </done>
</task>

<task type="auto">
  <name>Task 2: Export train_epoch_two_phase from src/__init__.py</name>
  <files>src/__init__.py</files>
  <action>
Add the new unified_training exports to `src/__init__.py`.

Add after the existing generalization_training imports:

```python
from .unified_training import (
    train_epoch_two_phase,
)
```

This makes the function available as `from src import train_epoch_two_phase`.
  </action>
  <verify>
Run: `python -c "from src import train_epoch_two_phase; print('Export OK')"`
Expected: "Export OK" printed without errors
  </verify>
  <done>
- `train_epoch_two_phase` exported from `src/__init__.py`
- Function importable via `from src import train_epoch_two_phase`
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify integration with existing modules</name>
  <files>None (verification only)</files>
  <action>
Run a quick integration test to verify the function works with real components:

```python
import torch
from src import Config, ParticleTorch, ModelAdapter, train_epoch_two_phase

# Create minimal test setup
config = Config(
    energy_threshold=0.1,  # 10% for fast test
    steps_per_epoch=3,
    replay_steps=10,
    replay_batch_size=2,
    min_replay_size=1,
)

# Simple 2-body particle
pos = torch.tensor([[0.5, 0.0], [-0.5, 0.0]], dtype=torch.float64)
vel = torch.tensor([[0.0, 1.0], [0.0, -1.0]], dtype=torch.float64)
mass = torch.tensor([1.0, 1.0], dtype=torch.float64)
particle = ParticleTorch.from_tensors(mass=mass, position=pos, velocity=vel, dt=0.001)

# Adapter and model
adapter = ModelAdapter(config)
input_dim = adapter.input_dim_from_state(particle)

# Simple model
model = torch.nn.Sequential(
    torch.nn.Linear(input_dim, 2, dtype=torch.float64),
)
torch.nn.init.constant_(model[0].weight, 0.0)
torch.nn.init.constant_(model[0].bias, 0.001)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Run one epoch
result = train_epoch_two_phase(model, particle, optimizer, config, adapter)

# Verify result structure
assert 'trajectory_metrics' in result
assert 'generalization_metrics' in result
assert 'converged' in result
assert 'part2_iterations' in result
assert 'epoch_time' in result
assert result['trajectory_metrics']['trajectory_length'] == 3  # steps_per_epoch
print("Integration test PASSED")
```
  </action>
  <verify>
Run the integration test script above.
Expected output: "Integration test PASSED"
  </verify>
  <done>
- `train_epoch_two_phase()` integrates correctly with ModelAdapter, ParticleTorch, Config
- Returns expected result structure with all required keys
- Trajectory length matches steps_per_epoch
  </done>
</task>

</tasks>

<verification>
1. `src/unified_training.py` exists with >= 80 lines
2. `train_epoch_two_phase()` function has correct signature
3. Function imports and calls `collect_trajectory()` and `generalize_on_trajectory()`
4. Function exported from `src/__init__.py`
5. Integration test passes with real components
</verification>

<success_criteria>
- train_epoch_two_phase() orchestrates Part 1 + Part 2 in sequence
- Trajectory from Part 1 passed directly to Part 2 (no transformation)
- Returns combined metrics from both parts
- Epoch timing tracked via epoch_time field
- Empty trajectory edge case handled with warning
- Function importable from src package
</success_criteria>

<output>
After completion, create `.planning/phases/05-unified-epoch-structure/05-01-SUMMARY.md`
</output>
