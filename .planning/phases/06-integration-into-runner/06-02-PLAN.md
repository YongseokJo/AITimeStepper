---
phase: 06-integration-into-runner
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - tests/test_runner_integration.py
autonomous: true

must_haves:
  truths:
    - "CLI train command works with same arguments as before"
    - "History-aware training (--history-len, --feature-type) works correctly"
    - "Checkpoint loads correctly in simulation mode"
    - "Multi-orbit warning is emitted when --num-orbits > 1"
    - "Duration warning is emitted when --duration is set"
    - "W&B logging works when --wandb flag provided"
  artifacts:
    - path: "tests/test_runner_integration.py"
      provides: "Integration tests for runner.py refactor"
      min_lines: 150
      exports: []
  key_links:
    - from: "tests/test_runner_integration.py"
      to: "run/runner.py"
      via: "subprocess calls to runner.py"
      pattern: "runner\\.py.*train"
    - from: "tests/test_runner_integration.py"
      to: "run/runner.py"
      via: "subprocess calls to runner.py simulate"
      pattern: "runner\\.py.*simulate"
---

<objective>
Create integration tests for the Phase 6 runner.py refactor to verify CLI compatibility, checkpoint contract, and W&B logging.

Purpose: Ensure the refactored run_training() function maintains full backward compatibility with the CLI interface and that the two-phase training system integrates correctly with existing workflows.

Output: New test file `tests/test_runner_integration.py` with integration tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-integration-into-runner/06-RESEARCH.md
@.planning/phases/06-integration-into-runner/06-01-SUMMARY.md

# Key source files
@run/runner.py
@tests/test_unified_training.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test file with CLI compatibility tests</name>
  <files>tests/test_runner_integration.py</files>
  <action>
Create `tests/test_runner_integration.py` with tests for CLI compatibility.

```python
"""
Integration tests for Phase 6: runner.py refactor.

Tests verify:
1. CLI train command works with existing arguments
2. Checkpoint contract preserved for simulation mode
3. Multi-orbit warning emitted
4. History-aware training works
5. W&B logging (mocked)

These tests run actual subprocess calls to runner.py to ensure
end-to-end CLI compatibility.
"""

import json
import os
import shutil
import subprocess
import sys
import tempfile
import warnings
from pathlib import Path
from unittest import mock

import pytest
import torch

# Add project root to path
project_root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_root))

from src import Config, run_two_phase_training, ModelAdapter, make_particle, FullyConnectedNN
from run.runner import run_training
from simulators.nbody_simulator import generate_random_ic


class TestCLICompatibility:
    """Test that CLI train command works with existing arguments."""

    @pytest.fixture
    def temp_save_dir(self):
        """Create temporary directory for test outputs."""
        tmpdir = tempfile.mkdtemp()
        yield tmpdir
        shutil.rmtree(tmpdir, ignore_errors=True)

    def test_basic_train_command(self, temp_save_dir):
        """Test: python run/runner.py train --epochs 5 --num-particles 3"""
        save_name = "test_basic_train"
        result = subprocess.run(
            [
                sys.executable, str(project_root / "run" / "runner.py"),
                "train",
                "--epochs", "5",
                "--num-particles", "3",
                "--save-name", save_name,
            ],
            capture_output=True,
            text=True,
            timeout=120,
            cwd=str(project_root),
        )

        # Training should complete successfully
        assert result.returncode == 0, f"Training failed: {result.stderr}"

        # Check for training summary in output
        assert "Training complete" in result.stdout, f"Missing summary. Output: {result.stdout}"
        assert "epochs" in result.stdout.lower()
        assert "convergence rate" in result.stdout.lower()

        # Check checkpoint exists
        checkpoint_dir = project_root / "data" / save_name / "model"
        assert checkpoint_dir.exists(), f"Checkpoint dir not created: {checkpoint_dir}"

        checkpoints = list(checkpoint_dir.glob("model_epoch_*.pt"))
        assert len(checkpoints) > 0, "No checkpoints saved"

        # Cleanup
        shutil.rmtree(project_root / "data" / save_name, ignore_errors=True)

    def test_train_with_history(self, temp_save_dir):
        """Test: train command with history-aware features."""
        save_name = "test_history_train"
        result = subprocess.run(
            [
                sys.executable, str(project_root / "run" / "runner.py"),
                "train",
                "--epochs", "5",
                "--num-particles", "3",
                "--history-len", "3",
                "--feature-type", "delta_mag",
                "--save-name", save_name,
            ],
            capture_output=True,
            text=True,
            timeout=120,
            cwd=str(project_root),
        )

        assert result.returncode == 0, f"History training failed: {result.stderr}"
        assert "Training complete" in result.stdout

        # Cleanup
        shutil.rmtree(project_root / "data" / save_name, ignore_errors=True)

    def test_train_with_steps_per_epoch(self, temp_save_dir):
        """Test: train with new steps_per_epoch parameter."""
        save_name = "test_steps_per_epoch"
        result = subprocess.run(
            [
                sys.executable, str(project_root / "run" / "runner.py"),
                "train",
                "--epochs", "3",
                "--num-particles", "3",
                "--steps-per-epoch", "5",
                "--save-name", save_name,
            ],
            capture_output=True,
            text=True,
            timeout=120,
            cwd=str(project_root),
        )

        assert result.returncode == 0, f"Training with steps_per_epoch failed: {result.stderr}"

        # Cleanup
        shutil.rmtree(project_root / "data" / save_name, ignore_errors=True)


class TestCheckpointContract:
    """Test that checkpoint contract is preserved for simulation mode."""

    @pytest.fixture
    def trained_checkpoint(self):
        """Train a model and return checkpoint path."""
        save_name = "test_checkpoint_contract"

        # Train
        result = subprocess.run(
            [
                sys.executable, str(project_root / "run" / "runner.py"),
                "train",
                "--epochs", "10",
                "--num-particles", "3",
                "--save-name", save_name,
            ],
            capture_output=True,
            text=True,
            timeout=120,
            cwd=str(project_root),
        )
        assert result.returncode == 0, f"Training failed: {result.stderr}"

        checkpoint_dir = project_root / "data" / save_name / "model"
        checkpoints = sorted(checkpoint_dir.glob("model_epoch_*.pt"))
        assert len(checkpoints) > 0, "No checkpoint created"

        yield checkpoints[-1]  # Return last checkpoint

        # Cleanup
        shutil.rmtree(project_root / "data" / save_name, ignore_errors=True)

    def test_checkpoint_has_required_fields(self, trained_checkpoint):
        """Verify checkpoint contains all required fields."""
        ckpt = torch.load(trained_checkpoint, map_location="cpu")

        # Required fields for model loading
        assert "model_state_dict" in ckpt or "model_state" in ckpt, "Missing model state"
        assert "epoch" in ckpt, "Missing epoch"

        # Config fields for simulation mode
        assert "config" in ckpt, "Missing config"
        config_data = ckpt["config"]
        assert "history_len" in config_data or ckpt.get("history_len") is not None, "Missing history_len"
        assert "feature_type" in config_data or ckpt.get("feature_type") is not None, "Missing feature_type"
        assert "dtype" in config_data or ckpt.get("dtype") is not None, "Missing dtype"

    def test_simulation_loads_checkpoint(self, trained_checkpoint):
        """Verify simulation mode can load and use checkpoint."""
        result = subprocess.run(
            [
                sys.executable, str(project_root / "run" / "runner.py"),
                "simulate",
                "--integrator-mode", "ml",
                "--model-path", str(trained_checkpoint),
                "--num-particles", "3",
                "--steps", "20",
            ],
            capture_output=True,
            text=True,
            timeout=60,
            cwd=str(project_root),
        )

        assert result.returncode == 0, f"Simulation failed: {result.stderr}"

        # Output should be valid JSON with energy residual
        output = result.stdout.strip()
        try:
            sim_result = json.loads(output)
        except json.JSONDecodeError:
            pytest.fail(f"Simulation output not valid JSON: {output}")

        assert "energy_residual" in sim_result, f"Missing energy_residual in: {sim_result}"
        assert "steps" in sim_result, f"Missing steps in: {sim_result}"

    def test_history_checkpoint_simulation(self):
        """Verify history-aware checkpoint works in simulation."""
        save_name = "test_history_checkpoint"

        # Train with history
        train_result = subprocess.run(
            [
                sys.executable, str(project_root / "run" / "runner.py"),
                "train",
                "--epochs", "10",
                "--num-particles", "3",
                "--history-len", "3",
                "--feature-type", "delta_mag",
                "--save-name", save_name,
            ],
            capture_output=True,
            text=True,
            timeout=120,
            cwd=str(project_root),
        )
        assert train_result.returncode == 0, f"Training failed: {train_result.stderr}"

        checkpoint_dir = project_root / "data" / save_name / "model"
        checkpoints = sorted(checkpoint_dir.glob("model_epoch_*.pt"))
        checkpoint = checkpoints[-1]

        # Simulate with history mode
        sim_result = subprocess.run(
            [
                sys.executable, str(project_root / "run" / "runner.py"),
                "simulate",
                "--integrator-mode", "history",
                "--model-path", str(checkpoint),
                "--num-particles", "3",
                "--steps", "20",
            ],
            capture_output=True,
            text=True,
            timeout=60,
            cwd=str(project_root),
        )

        assert sim_result.returncode == 0, f"History simulation failed: {sim_result.stderr}"

        # Cleanup
        shutil.rmtree(project_root / "data" / save_name, ignore_errors=True)


class TestMultiOrbitWarning:
    """Test that multi-orbit warning is emitted."""

    def test_multi_orbit_warning_emitted(self):
        """Verify warning when num_orbits > 1."""
        # Create config with num_orbits > 1
        config = Config(
            epochs=1,
            num_particles=3,
            num_orbits=4,  # Should trigger warning
            save_name="test_multi_orbit_warning",
        )

        # Capture warnings
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")

            # Call run_training directly (faster than subprocess)
            try:
                run_training(config)
            except Exception:
                pass  # May fail for other reasons, we just want to check warning

            # Check for multi-orbit warning
            multi_orbit_warnings = [
                warning for warning in w
                if "Multi-orbit" in str(warning.message) or "num_orbits" in str(warning.message)
            ]
            assert len(multi_orbit_warnings) > 0, f"No multi-orbit warning. Warnings: {[str(x.message) for x in w]}"

        # Cleanup
        shutil.rmtree(project_root / "data" / "test_multi_orbit_warning", ignore_errors=True)

    def test_multi_orbit_cli_warning(self):
        """Test multi-orbit warning via CLI."""
        save_name = "test_multi_orbit_cli"
        result = subprocess.run(
            [
                sys.executable, str(project_root / "run" / "runner.py"),
                "train",
                "--epochs", "2",
                "--num-particles", "3",
                "--num-orbits", "4",
                "--save-name", save_name,
            ],
            capture_output=True,
            text=True,
            timeout=120,
            cwd=str(project_root),
        )

        # Should complete (with warning, not error)
        assert result.returncode == 0, f"Training failed: {result.stderr}"

        # Warning should appear in stderr
        assert "Multi-orbit" in result.stderr or "num_orbits" in result.stderr, \
            f"No multi-orbit warning in stderr: {result.stderr}"

        # Cleanup
        shutil.rmtree(project_root / "data" / save_name, ignore_errors=True)


class TestDurationWarning:
    """Test that duration warning is emitted."""

    def test_duration_warning_emitted(self):
        """Verify warning when duration is set."""
        config = Config(
            epochs=1,
            num_particles=3,
            duration=60.0,  # Should trigger warning
            save_name="test_duration_warning",
        )

        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")

            try:
                run_training(config)
            except Exception:
                pass  # May fail for other reasons, we just want to check warning

            # Check for duration warning
            duration_warnings = [
                warning for warning in w
                if "duration" in str(warning.message).lower()
            ]
            assert len(duration_warnings) > 0, f"No duration warning. Warnings: {[str(x.message) for x in w]}"

        # Cleanup
        shutil.rmtree(project_root / "data" / "test_duration_warning", ignore_errors=True)


class TestWandBLogging:
    """Test W&B logging compatibility (mocked)."""

    def test_wandb_flag_accepted(self):
        """Test that --wandb flag is accepted (will fail if wandb not installed, that's OK)."""
        save_name = "test_wandb_flag"
        result = subprocess.run(
            [
                sys.executable, str(project_root / "run" / "runner.py"),
                "train",
                "--epochs", "2",
                "--num-particles", "3",
                "--wandb",  # This flag should be accepted
                "--save-name", save_name,
            ],
            capture_output=True,
            text=True,
            timeout=60,
            cwd=str(project_root),
            env={**os.environ, "WANDB_MODE": "disabled"},  # Disable actual W&B
        )

        # Either succeeds or fails with "wandb not installed" - both are valid
        # The important thing is that the flag is recognized
        assert "unrecognized arguments: --wandb" not in result.stderr, \
            "--wandb flag not recognized"

        # Cleanup
        shutil.rmtree(project_root / "data" / save_name, ignore_errors=True)


class TestDirectFunctionCalls:
    """Test run_training() function directly (faster than subprocess)."""

    def test_run_training_basic(self):
        """Test run_training() with minimal config."""
        config = Config(
            epochs=3,
            num_particles=3,
            steps_per_epoch=2,
            energy_threshold=0.1,  # Looser for faster test
            save_name="test_direct_basic",
        )

        # Should complete without error
        run_training(config)

        # Check checkpoint exists
        checkpoint_dir = project_root / "data" / config.save_name / "model"
        assert checkpoint_dir.exists()

        # Cleanup
        shutil.rmtree(project_root / "data" / config.save_name, ignore_errors=True)

    def test_run_training_with_history(self):
        """Test run_training() with history features."""
        config = Config(
            epochs=3,
            num_particles=3,
            steps_per_epoch=3,
            history_len=2,
            feature_type="delta_mag",
            energy_threshold=0.1,
            save_name="test_direct_history",
        )

        run_training(config)

        checkpoint_dir = project_root / "data" / config.save_name / "model"
        assert checkpoint_dir.exists()

        # Cleanup
        shutil.rmtree(project_root / "data" / config.save_name, ignore_errors=True)


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
```

Key test categories:
1. **TestCLICompatibility**: Verify CLI arguments work (--epochs, --num-particles, --history-len, etc.)
2. **TestCheckpointContract**: Verify checkpoints have required fields and work with simulation mode
3. **TestMultiOrbitWarning**: Verify warning emitted when num_orbits > 1
4. **TestWandBLogging**: Verify --wandb flag is accepted
5. **TestDirectFunctionCalls**: Fast unit tests calling run_training() directly
  </action>
  <verify>
Run: `pytest tests/test_runner_integration.py -v --tb=short -x`
Expected: All tests pass (may take 2-3 minutes due to subprocess training calls)
  </verify>
  <done>
- Test file created with CLI compatibility tests
- Checkpoint contract tests verify required fields
- Multi-orbit warning test confirms warning emitted
- W&B flag acceptance test (mocked)
- Direct function call tests for faster verification
  </done>
</task>

<task type="auto">
  <name>Task 2: Run integration tests and verify all pass</name>
  <files>None (verification only)</files>
  <action>
Run the full integration test suite:

```bash
# Run all integration tests
pytest tests/test_runner_integration.py -v --tb=short

# Expected output:
# - TestCLICompatibility: 3 tests pass
# - TestCheckpointContract: 3-4 tests pass
# - TestMultiOrbitWarning: 2 tests pass
# - TestWandBLogging: 1 test pass
# - TestDirectFunctionCalls: 2 tests pass
```

If any tests fail:
1. Check stderr output for error messages
2. Verify run_training() changes from Plan 06-01 are correct
3. Check that checkpoint format matches expected structure
  </action>
  <verify>
All tests in `tests/test_runner_integration.py` should pass.
Acceptable: W&B test may skip if wandb not installed.
  </verify>
  <done>
- All CLI compatibility tests pass
- Checkpoint contract tests pass
- Multi-orbit warning tests pass
- Integration test suite provides regression protection
  </done>
</task>

</tasks>

<verification>
1. `tests/test_runner_integration.py` exists with >= 150 lines
2. CLI compatibility tests verify existing arguments work
3. Checkpoint contract tests verify simulation mode compatibility
4. Multi-orbit warning test confirms UserWarning emitted
5. Duration warning test confirms UserWarning emitted
6. All tests pass (W&B may skip if not installed)
</verification>

<success_criteria>
- CLI train command works with same arguments as before
- History-aware training (--history-len, --feature-type) works
- Checkpoint loads correctly in simulation mode
- Multi-orbit warning emitted when --num-orbits > 1
- Duration warning emitted when --duration is set
- Integration test suite provides confidence in refactor
</success_criteria>

<output>
After completion, create `.planning/phases/06-integration-into-runner/06-02-SUMMARY.md`
</output>
