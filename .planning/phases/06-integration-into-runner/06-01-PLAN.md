---
phase: 06-integration-into-runner
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - run/runner.py
autonomous: true

must_haves:
  truths:
    - "run_training() calls run_two_phase_training() instead of manual training loop"
    - "Multi-orbit warning issued when num_orbits > 1"
    - "Training summary printed after run_two_phase_training() returns"
    - "W&B finish() still called after training completes"
    - "Existing setup code (device, dtype, seeds, adapter, particle, model, optimizer) unchanged"
  artifacts:
    - path: "run/runner.py"
      provides: "Refactored run_training() function"
      min_lines: 400
      contains: "run_two_phase_training"
  key_links:
    - from: "run/runner.py"
      to: "src/unified_training.py"
      via: "import run_two_phase_training"
      pattern: "from src import.*run_two_phase_training"
    - from: "run/runner.py"
      to: "run_two_phase_training call"
      via: "function invocation"
      pattern: "run_two_phase_training\\("
---

<objective>
Refactor `run_training()` in `run/runner.py` to use the new two-phase training system by calling `run_two_phase_training()` instead of the manual training loop.

Purpose: This is the core integration task. Replace the existing epoch loop (lines 313-391) with a single call to `run_two_phase_training()`, preserving all setup code and adding appropriate warnings for unsupported features.

Output: Modified `run/runner.py` with refactored `run_training()` function.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-integration-into-runner/06-RESEARCH.md

# Key source files
@run/runner.py
@src/unified_training.py
@src/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update imports in runner.py</name>
  <files>run/runner.py</files>
  <action>
Modify the imports at the top of `run/runner.py`.

Add `run_two_phase_training` to the import from src.

Change the import block (around line 14-27) from:

```python
from src import (
    Config,
    FullyConnectedNN,
    HistoryBuffer,
    ModelAdapter,
    load_model_state,
    load_config_from_checkpoint,
    loss_fn_batch,
    loss_fn_batch_history,
    loss_fn_batch_history_batch,
    make_particle,
    save_checkpoint,
    stack_particles,
)
```

To:

```python
from src import (
    Config,
    FullyConnectedNN,
    HistoryBuffer,
    ModelAdapter,
    load_model_state,
    load_config_from_checkpoint,
    loss_fn_batch,
    loss_fn_batch_history,
    loss_fn_batch_history_batch,
    make_particle,
    save_checkpoint,
    stack_particles,
    run_two_phase_training,
)
```

Also add `import warnings` at the top with the other standard library imports (around line 4-6) if not already present.

NOTE: Keep the old loss function imports for now. They will be cleaned up in Phase 7 after confirming no other code uses them.
  </action>
  <verify>
Run: `python -c "from run.runner import run_training; print('Import OK')"`
Expected: "Import OK" printed without errors
  </verify>
  <done>
- `run_two_phase_training` imported from src
- `warnings` module imported
- All existing imports preserved (cleanup in Phase 7)
  </done>
</task>

<task type="auto">
  <name>Task 2: Refactor run_training() to use run_two_phase_training()</name>
  <files>run/runner.py</files>
  <action>
Replace the training loop section in `run_training()` (lines 313-391) with a call to `run_two_phase_training()`.

The refactored `run_training()` function should follow this structure:

```python
def run_training(config: Config) -> None:
    """Train ML time-stepper using two-phase training system."""

    # === SECTION 1: VALIDATION (unchanged) ===
    if config.integrator_mode == "history" and (config.history_len is None or config.history_len <= 0):
        raise ValueError("history integrator_mode requires history_len > 0")
    config.validate()

    # === SECTION 2: DEVICE/DTYPE SETUP (unchanged) ===
    device = config.resolve_device()
    dtype = config.resolve_dtype()
    torch.set_default_dtype(dtype)
    config.apply_torch_settings(device)

    # === SECTION 3: SEED (unchanged) ===
    if config.seed is not None:
        torch.manual_seed(config.seed)
        np.random.seed(config.seed)

    # === SECTION 4: ADAPTER (unchanged) ===
    adapter = ModelAdapter(config, device=device, dtype=dtype)

    # === SECTION 5: W&B SETUP (unchanged) ===
    wandb_run = None
    wandb = None
    if config.extra.get("wandb", False):
        try:
            import wandb as wandb_lib
        except ImportError as exc:
            raise RuntimeError("wandb is not installed; install it or disable --wandb") from exc
        wandb = wandb_lib
        wandb_project = config.extra.get("wandb_project") or "AITimeStepper"
        wandb_name = config.extra.get("wandb_name") or config.save_name or "runner_train"
        wandb_run = wandb.init(
            project=wandb_project,
            name=wandb_name,
            config=config.as_wandb_dict(),
        )

    # === SECTION 6: MULTI-ORBIT CHECK (NEW) ===
    if config.num_orbits > 1:
        warnings.warn(
            f"Multi-orbit training (num_orbits={config.num_orbits}) not yet supported "
            "in two-phase training. Using single orbit.",
            UserWarning,
        )

    # === SECTION 7: PARTICLE INITIALIZATION (simplified - single orbit only) ===
    ptcls = generate_random_ic(
        num_particles=config.num_particles,
        dim=config.dim,
        mass=config.mass,
        pos_scale=config.pos_scale,
        vel_scale=config.vel_scale,
        seed=config.seed,
    )
    particle = make_particle(ptcls, device=device, dtype=dtype)
    particle.current_time = torch.tensor(0.0, device=device, dtype=dtype)
    input_dim = adapter.input_dim_from_state(particle, history_buffer=adapter.history_buffer)

    # === SECTION 8: MODEL CREATION (unchanged) ===
    model = FullyConnectedNN(
        input_dim=input_dim,
        output_dim=2,
        hidden_dims=[200, 1000, 1000, 200],
        activation="tanh",
        dropout=0.2,
        output_positive=True,
    ).to(device)
    model.to(dtype=dtype)

    # === SECTION 9: OPTIMIZER (unchanged) ===
    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)

    # === SECTION 10: TRAINING (NEW - two-phase system) ===
    save_dir = pathlib.Path(project_root) / "data" / (config.save_name or "run_nbody") / "model"

    result = run_two_phase_training(
        model=model,
        particle=particle,
        optimizer=optimizer,
        config=config,
        adapter=adapter,
        history_buffer=adapter.history_buffer,
        save_dir=save_dir,
        wandb_run=wandb_run,
        checkpoint_interval=10,
    )

    # === SECTION 11: TRAINING SUMMARY (NEW) ===
    print(f"\nTraining complete: {result['epochs_completed']} epochs, "
          f"convergence rate: {result['convergence_rate']:.1%}, "
          f"total time: {result['total_time']:.1f}s")

    # === SECTION 12: W&B CLEANUP (unchanged) ===
    if wandb_run is not None:
        wandb.finish()
```

Key changes:
1. Remove the `_wandb_log_value()` helper function (no longer needed - run_two_phase_training handles logging)
2. Remove the `_build_particle()` inner function (simplify to inline code)
3. Remove the multi-orbit particle initialization block (replaced with warning + single orbit)
4. Remove the training loop (while epoch < config.epochs: ...)
5. Add call to `run_two_phase_training()` with correct arguments
6. Add training summary print after training completes

NOTE: Remove the `start_time = time.perf_counter()` line that was only used for the old duration check - `run_two_phase_training()` handles timing internally.
  </action>
  <verify>
Run: `python run/runner.py train --epochs 5 --num-particles 3 --save-name test_phase6_refactor`
Expected: Training runs successfully, prints progress every 10 epochs, prints final summary
  </verify>
  <done>
- run_training() calls run_two_phase_training() instead of manual loop
- Multi-orbit warning issued when num_orbits > 1
- Training summary printed after completion
- Setup code (device, dtype, seeds, adapter, model, optimizer) unchanged
- W&B cleanup still runs after training
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify checkpoint contract preserved</name>
  <files>None (verification only)</files>
  <action>
Verify that checkpoints saved by the new training loop can be loaded by simulation mode.

Run this test sequence:

```bash
# Step 1: Train with new system (short run)
python run/runner.py train --epochs 10 --num-particles 3 --save-name test_checkpoint_compat

# Step 2: Check checkpoint exists
ls data/test_checkpoint_compat/model/

# Step 3: Load checkpoint in simulation mode (ml integrator)
python run/runner.py simulate --integrator-mode ml --model-path data/test_checkpoint_compat/model/model_epoch_0009.pt --num-particles 3 --steps 50

# Step 4: Verify simulation output (should print JSON with energy_residual)
```

The checkpoint contract requires:
- `model_state_dict` present in checkpoint
- `config` section with `history_len`, `feature_type`, `dtype`
- Simulation mode can load and use the checkpoint
  </action>
  <verify>
Step 3 should complete without errors and print JSON output with energy/momentum residuals.
  </verify>
  <done>
- Checkpoint saved with correct format
- Simulation mode loads checkpoint successfully
- Config fields (history_len, feature_type, dtype) preserved
  </done>
</task>

</tasks>

<verification>
1. `run/runner.py` imports `run_two_phase_training` from src
2. `run_training()` function calls `run_two_phase_training()` with correct arguments
3. Multi-orbit warning issued when `num_orbits > 1`
4. Training summary printed after completion
5. Checkpoint contract preserved - simulation mode loads new checkpoints
6. W&B cleanup still runs (`wandb.finish()`)
</verification>

<success_criteria>
- run_training() delegates to run_two_phase_training() for training loop
- Existing Config fields (history_len, feature_type, num_orbits) still supported
- Multi-orbit warning issued (graceful degradation, not error)
- CLI interface unchanged (same arguments work)
- Checkpoint backward compatible with simulation mode
- Training summary shows epochs, convergence rate, time
</success_criteria>

<output>
After completion, create `.planning/phases/06-integration-into-runner/06-01-SUMMARY.md`
</output>
