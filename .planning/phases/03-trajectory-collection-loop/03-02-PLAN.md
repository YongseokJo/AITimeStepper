---
phase: 03-trajectory-collection-loop
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/trajectory_collection.py
autonomous: true

must_haves:
  truths:
    - "Retrain loop continues until energy threshold satisfied"
    - "Each iteration performs optimizer.zero_grad(), loss.backward(), optimizer.step()"
    - "Fresh clone created at start of each attempt"
    - "Returns accepted particle state and metrics"
  artifacts:
    - path: "src/trajectory_collection.py"
      provides: "collect_trajectory_step function"
      exports: ["collect_trajectory_step"]
      min_lines: 100
  key_links:
    - from: "src/trajectory_collection.py:collect_trajectory_step"
      to: "src/trajectory_collection.py:attempt_single_step"
      via: "function call in loop"
      pattern: "attempt_single_step"
    - from: "src/trajectory_collection.py:collect_trajectory_step"
      to: "torch.optim"
      via: "optimizer step"
      pattern: "optimizer\\.step"
---

<objective>
Implement the accept/reject retrain loop that collects a single validated trajectory step.

Purpose: This is the core of TRAIN-01, TRAIN-02, TRAIN-03 requirements. The loop predicts dt, checks energy, and retrains until the step passes the energy threshold.

Output: `collect_trajectory_step()` function that returns an accepted (state, dt) pair after potentially multiple retrain iterations.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-trajectory-collection-loop/03-RESEARCH.md
@.planning/phases/03-trajectory-collection-loop/03-01-SUMMARY.md

# Source files
@src/trajectory_collection.py
@src/particle.py
@src/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement collect_trajectory_step function</name>
  <files>src/trajectory_collection.py</files>
  <action>
Add the main accept/reject loop function to `src/trajectory_collection.py`:

```python
def collect_trajectory_step(
    model: torch.nn.Module,
    particle: ParticleTorch,
    optimizer: torch.optim.Optimizer,
    config: Config,
    adapter: ModelAdapter,
    history_buffer: Optional[HistoryBuffer] = None,
) -> Tuple[ParticleTorch, float, Dict[str, Any]]:
    """
    Collect one accepted trajectory step with retrain loop.

    Implements TRAIN-01, TRAIN-02, TRAIN-03:
    - Predict dt, integrate one step, check energy
    - If energy exceeds threshold: reject, retrain on same state
    - Loop until energy within threshold
    - Return accepted particle state and dt

    IMPORTANT: No retry limit by design (user requirement).
    For debugging, check metrics['retrain_iterations'].

    Args:
        model: Neural network for dt prediction
        particle: Starting particle state
        optimizer: PyTorch optimizer for model
        config: Config with energy_threshold
        adapter: ModelAdapter for feature construction
        history_buffer: Optional history buffer

    Returns:
        (accepted_particle, accepted_dt, metrics) where:
        - accepted_particle: ParticleTorch that passed energy check
        - accepted_dt: float value of accepted timestep
        - metrics: dict with 'retrain_iterations', 'final_energy_error'
    """
```

Implementation:
```python
retrain_iterations = 0

while True:
    # 1. Attempt a step (creates fresh clone internally)
    p_attempt, dt, E0, E1 = attempt_single_step(
        model, particle, config, adapter, history_buffer
    )

    # 2. Check energy threshold
    passed, rel_dE = check_energy_threshold(E0, E1, config.energy_threshold)

    if passed:
        # ACCEPT: energy within threshold
        metrics = {
            'retrain_iterations': retrain_iterations,
            'final_energy_error': rel_dE.item() if rel_dE.numel() == 1 else rel_dE.mean().item(),
        }
        # Extract dt as float for return
        dt_value = dt.item() if dt.numel() == 1 else dt.mean().item()
        return p_attempt, dt_value, metrics

    # REJECT: retrain on same state
    loss = compute_single_step_loss(E0, E1, config)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    retrain_iterations += 1
```

Key points:
- No max iteration limit (per user requirement)
- Fresh clone at each attempt (inside attempt_single_step)
- Use optimizer passed as argument
- Return detached particle for storage
  </action>
  <verify>
Run: `python -c "from src.trajectory_collection import collect_trajectory_step; print('collect_trajectory_step OK')"`
  </verify>
  <done>Function `collect_trajectory_step` implements accept/reject loop with retrain</done>
</task>

<task type="auto">
  <name>Task 2: Add diagnostic logging option</name>
  <files>src/trajectory_collection.py</files>
  <action>
Add optional diagnostic logging to help debug stuck retrain loops:

1. Add optional `max_retrain_warn` parameter (default=1000):
```python
def collect_trajectory_step(
    model: torch.nn.Module,
    particle: ParticleTorch,
    optimizer: torch.optim.Optimizer,
    config: Config,
    adapter: ModelAdapter,
    history_buffer: Optional[HistoryBuffer] = None,
    max_retrain_warn: int = 1000,
) -> Tuple[ParticleTorch, float, Dict[str, Any]]:
```

2. Inside the loop, add warning logging:
```python
if retrain_iterations > 0 and retrain_iterations % max_retrain_warn == 0:
    import warnings
    warnings.warn(
        f"collect_trajectory_step: {retrain_iterations} retrain iterations "
        f"(rel_dE={rel_dE.item():.2e}, threshold={config.energy_threshold:.2e})",
        RuntimeWarning,
    )
```

This allows monitoring without breaking the infinite loop requirement.

3. Add 'reject_count' to metrics (same as retrain_iterations for clarity):
```python
metrics = {
    'retrain_iterations': retrain_iterations,
    'reject_count': retrain_iterations,
    'final_energy_error': rel_dE.item() if rel_dE.numel() == 1 else rel_dE.mean().item(),
}
```
  </action>
  <verify>
Run: `python -c "from src.trajectory_collection import collect_trajectory_step; import inspect; sig = inspect.signature(collect_trajectory_step); print('max_retrain_warn' in sig.parameters)"`
  </verify>
  <done>Diagnostic warning added for long-running retrain loops</done>
</task>

<task type="auto">
  <name>Task 3: Export function and update __init__.py</name>
  <files>src/__init__.py</files>
  <action>
Ensure `collect_trajectory_step` is exported from the src package:

1. Add to existing imports in `src/__init__.py`:
```python
from .trajectory_collection import (
    attempt_single_step,
    check_energy_threshold,
    compute_single_step_loss,
    collect_trajectory_step,
)
```

2. Add to `__all__` if it exists, or ensure all functions are accessible.

Verify the full import chain works from the project root.
  </action>
  <verify>
Run: `cd /u/gkerex/projects/AITimeStepper && python -c "from src import collect_trajectory_step; print('Export OK')"`
  </verify>
  <done>All trajectory collection functions exported from src package</done>
</task>

</tasks>

<verification>
1. Import test: `python -c "from src import collect_trajectory_step"`
2. Signature check: `python -c "from src import collect_trajectory_step; import inspect; print(inspect.signature(collect_trajectory_step))"`
3. Docstring check: `python -c "from src import collect_trajectory_step; print(collect_trajectory_step.__doc__[:200])"`
</verification>

<success_criteria>
1. `collect_trajectory_step` exists with correct signature
2. Function implements while True loop with accept/reject logic
3. Uses `attempt_single_step` and `check_energy_threshold` from Plan 01
4. Calls optimizer.zero_grad(), loss.backward(), optimizer.step() on reject
5. Returns (particle, dt_float, metrics_dict)
6. No max iteration limit (infinite loop until success)
7. Warning issued every 1000 iterations for debugging
</success_criteria>

<output>
After completion, create `.planning/phases/03-trajectory-collection-loop/03-02-SUMMARY.md`
</output>
