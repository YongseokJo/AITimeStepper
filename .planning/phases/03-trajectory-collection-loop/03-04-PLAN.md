---
phase: 03-trajectory-collection-loop
plan: 04
type: execute
wave: 3
depends_on: ["03-02", "03-03"]
files_modified:
  - tests/test_trajectory_collection.py
autonomous: true

must_haves:
  truths:
    - "Tests verify attempt_single_step returns correct tuple"
    - "Tests verify check_energy_threshold accepts/rejects correctly"
    - "Tests verify collect_trajectory_step retrains until passing"
    - "Tests verify collect_trajectory discards warmup steps"
  artifacts:
    - path: "tests/test_trajectory_collection.py"
      provides: "Unit tests for trajectory collection"
      min_lines: 150
  key_links:
    - from: "tests/test_trajectory_collection.py"
      to: "src/trajectory_collection.py"
      via: "import and test functions"
      pattern: "from src.trajectory_collection import"
---

<objective>
Create comprehensive unit tests for the trajectory collection module.

Purpose: Verify all trajectory collection functions work correctly with mock models and particles. Tests should cover accept/reject logic, warmup discard, and edge cases.

Output: `tests/test_trajectory_collection.py` with pytest-based tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-trajectory-collection-loop/03-RESEARCH.md
@.planning/phases/03-trajectory-collection-loop/03-01-SUMMARY.md
@.planning/phases/03-trajectory-collection-loop/03-02-SUMMARY.md
@.planning/phases/03-trajectory-collection-loop/03-03-SUMMARY.md

# Source files
@src/trajectory_collection.py
@src/particle.py
@src/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test file with fixtures</name>
  <files>tests/test_trajectory_collection.py</files>
  <action>
Create `tests/test_trajectory_collection.py` with test fixtures:

```python
"""
Tests for trajectory collection functions.

Covers:
- attempt_single_step
- check_energy_threshold
- compute_single_step_loss
- collect_trajectory_step
- collect_trajectory
"""

import pytest
import torch
import torch.nn as nn

from src.config import Config
from src.particle import ParticleTorch
from src.model_adapter import ModelAdapter
from src.history_buffer import HistoryBuffer
from src.trajectory_collection import (
    attempt_single_step,
    check_energy_threshold,
    compute_single_step_loss,
    collect_trajectory_step,
    collect_trajectory,
)


class MockModel(nn.Module):
    """Mock model that returns configurable dt values."""

    def __init__(self, dt_value: float = 0.001):
        super().__init__()
        self.dt_value = dt_value
        self.linear = nn.Linear(1, 2)  # Minimal trainable params

    def forward(self, x):
        batch_size = x.shape[0]
        # Return [dt, E_hat] for each sample
        return torch.full((batch_size, 2), self.dt_value, device=x.device, dtype=x.dtype)


class TrainableMockModel(nn.Module):
    """Mock model with trainable parameters for testing optimizer."""

    def __init__(self, input_dim: int = 11, initial_dt: float = 0.1):
        super().__init__()
        self.fc = nn.Linear(input_dim, 2)
        # Initialize to output approximately initial_dt
        nn.init.constant_(self.fc.weight, 0.0)
        nn.init.constant_(self.fc.bias, initial_dt)

    def forward(self, x):
        return torch.abs(self.fc(x)) + 1e-6  # Ensure positive


@pytest.fixture
def simple_particle():
    """Create a simple 2-body particle system."""
    pos = torch.tensor([[0.5, 0.0], [-0.5, 0.0]], dtype=torch.float64)
    vel = torch.tensor([[0.0, 1.0], [0.0, -1.0]], dtype=torch.float64)
    mass = torch.tensor([1.0, 1.0], dtype=torch.float64)
    return ParticleTorch.from_tensors(mass=mass, position=pos, velocity=vel, dt=0.001)


@pytest.fixture
def config():
    """Create a test configuration."""
    return Config(
        energy_threshold=2e-4,
        steps_per_epoch=5,
        history_len=0,
        feature_type="basic",
        E_lower=1e-6,
        E_upper=1e-4,
        num_particles=2,
        dim=2,
    )


@pytest.fixture
def config_with_history():
    """Create a test configuration with history enabled."""
    return Config(
        energy_threshold=2e-4,
        steps_per_epoch=5,
        history_len=3,
        feature_type="basic",
        E_lower=1e-6,
        E_upper=1e-4,
        num_particles=2,
        dim=2,
    )


@pytest.fixture
def adapter(config):
    """Create a model adapter."""
    return ModelAdapter(config, device=torch.device("cpu"), dtype=torch.float64)


@pytest.fixture
def adapter_with_history(config_with_history):
    """Create a model adapter with history."""
    return ModelAdapter(config_with_history, device=torch.device("cpu"), dtype=torch.float64)
```
  </action>
  <verify>
Run: `python -c "import tests.test_trajectory_collection; print('Fixtures OK')"`
  </verify>
  <done>Test file created with mock model and fixtures</done>
</task>

<task type="auto">
  <name>Task 2: Add tests for core functions</name>
  <files>tests/test_trajectory_collection.py</files>
  <action>
Add tests for the core functions:

```python
class TestAttemptSingleStep:
    """Tests for attempt_single_step function."""

    def test_returns_correct_tuple(self, simple_particle, config, adapter):
        """Verify return value structure."""
        model = MockModel(dt_value=0.0001)
        result = attempt_single_step(model, simple_particle, config, adapter)

        assert len(result) == 4, "Should return (particle, dt, E0, E1)"
        p, dt, E0, E1 = result

        assert isinstance(p, ParticleTorch)
        assert torch.is_tensor(dt)
        assert torch.is_tensor(E0)
        assert torch.is_tensor(E1)

    def test_clone_detached_at_start(self, simple_particle, config, adapter):
        """Verify particle is cloned to avoid modifying original."""
        model = MockModel(dt_value=0.0001)
        original_pos = simple_particle.position.clone()

        _ = attempt_single_step(model, simple_particle, config, adapter)

        # Original particle should be unchanged
        assert torch.allclose(simple_particle.position, original_pos)

    def test_small_dt_preserves_energy(self, simple_particle, config, adapter):
        """Small dt should give small energy change."""
        model = MockModel(dt_value=1e-6)
        _, dt, E0, E1 = attempt_single_step(model, simple_particle, config, adapter)

        rel_dE = torch.abs((E1 - E0) / (E0 + 1e-12))
        assert rel_dE.item() < 1e-3, "Small dt should preserve energy"


class TestCheckEnergyThreshold:
    """Tests for check_energy_threshold function."""

    def test_passes_when_below_threshold(self):
        """Should pass when relative error is below threshold."""
        E0 = torch.tensor([-1.0])
        E1 = torch.tensor([-1.0001])  # 0.01% error
        passed, rel_dE = check_energy_threshold(E0, E1, threshold=0.001)

        assert passed is True
        assert rel_dE.item() < 0.001

    def test_fails_when_above_threshold(self):
        """Should fail when relative error exceeds threshold."""
        E0 = torch.tensor([-1.0])
        E1 = torch.tensor([-1.01])  # 1% error
        passed, rel_dE = check_energy_threshold(E0, E1, threshold=0.001)

        assert passed is False
        assert rel_dE.item() > 0.001

    def test_handles_small_energy(self):
        """Should handle near-zero energy safely."""
        E0 = torch.tensor([1e-10])
        E1 = torch.tensor([1.1e-10])
        passed, rel_dE = check_energy_threshold(E0, E1, threshold=0.5)

        assert torch.isfinite(rel_dE)


class TestComputeSingleStepLoss:
    """Tests for compute_single_step_loss function."""

    def test_returns_scalar(self, config):
        """Loss should be a scalar tensor."""
        E0 = torch.tensor([-1.0])
        E1 = torch.tensor([-1.0001])
        loss = compute_single_step_loss(E0, E1, config)

        assert loss.dim() == 0, "Loss should be scalar"
        assert loss.item() >= 0, "Loss should be non-negative"

    def test_zero_loss_inside_band(self, config):
        """Loss should be zero when error is inside acceptable band."""
        E0 = torch.tensor([-1.0])
        # Error between E_lower and E_upper
        error = (config.E_lower + config.E_upper) / 2
        E1 = E0 * (1 + error)
        loss = compute_single_step_loss(E0, E1, config)

        assert loss.item() < 1e-6, "Loss should be ~zero inside band"
```
  </action>
  <verify>
Run: `cd /u/gkerex/projects/AITimeStepper && python -m pytest tests/test_trajectory_collection.py::TestAttemptSingleStep -v --tb=short 2>/dev/null || echo "Tests defined"`
  </verify>
  <done>Core function tests added</done>
</task>

<task type="auto">
  <name>Task 3: Add tests for collect_trajectory_step and collect_trajectory</name>
  <files>tests/test_trajectory_collection.py</files>
  <action>
Add tests for the orchestration functions:

```python
class TestCollectTrajectoryStep:
    """Tests for collect_trajectory_step function."""

    def test_returns_accepted_step(self, simple_particle, config, adapter):
        """Should return accepted particle, dt, and metrics."""
        model = TrainableMockModel(input_dim=11, initial_dt=1e-5)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        particle, dt, metrics = collect_trajectory_step(
            model, simple_particle, optimizer, config, adapter
        )

        assert isinstance(particle, ParticleTorch)
        assert isinstance(dt, float)
        assert isinstance(metrics, dict)
        assert 'retrain_iterations' in metrics
        assert 'final_energy_error' in metrics

    def test_energy_below_threshold(self, simple_particle, config, adapter):
        """Returned step should satisfy energy threshold."""
        # Use very small dt to ensure quick acceptance
        model = MockModel(dt_value=1e-7)
        optimizer = torch.optim.Adam([torch.nn.Parameter(torch.zeros(1))], lr=0.01)

        particle, dt, metrics = collect_trajectory_step(
            model, simple_particle, optimizer, config, adapter
        )

        assert metrics['final_energy_error'] < config.energy_threshold

    def test_retrains_until_passing(self, simple_particle, config, adapter):
        """Should retrain if initial prediction fails."""
        # Start with large dt that will fail
        model = TrainableMockModel(input_dim=11, initial_dt=0.1)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

        # Lower threshold to make it harder
        config_strict = Config(
            energy_threshold=1e-6,
            E_lower=1e-8,
            E_upper=1e-6,
            num_particles=2,
            dim=2,
        )

        # This might take many iterations, but should eventually pass
        particle, dt, metrics = collect_trajectory_step(
            model, simple_particle, optimizer, config_strict, adapter,
            max_retrain_warn=10000,  # Suppress warnings for test
        )

        # If it returns, it passed
        assert metrics['final_energy_error'] < config_strict.energy_threshold


class TestCollectTrajectory:
    """Tests for collect_trajectory function."""

    def test_collects_n_steps(self, simple_particle, config, adapter):
        """Should collect steps_per_epoch steps."""
        model = MockModel(dt_value=1e-6)
        optimizer = torch.optim.Adam([torch.nn.Parameter(torch.zeros(1))], lr=0.01)

        trajectory, epoch_metrics = collect_trajectory(
            model, simple_particle, optimizer, config, adapter
        )

        # Without history, no warmup discard
        assert len(trajectory) == config.steps_per_epoch
        assert epoch_metrics['total_steps'] == config.steps_per_epoch
        assert epoch_metrics['warmup_discarded'] == 0

    def test_discards_warmup_with_history(self, simple_particle, config_with_history, adapter_with_history):
        """Should discard first history_len steps."""
        model = MockModel(dt_value=1e-6)
        optimizer = torch.optim.Adam([torch.nn.Parameter(torch.zeros(1))], lr=0.01)

        trajectory, epoch_metrics = collect_trajectory(
            model, simple_particle, optimizer, config_with_history, adapter_with_history,
            history_buffer=adapter_with_history.history_buffer,
        )

        expected_len = config_with_history.steps_per_epoch - config_with_history.history_len
        assert len(trajectory) == max(0, expected_len)
        assert epoch_metrics['warmup_discarded'] == config_with_history.history_len

    def test_updates_history_buffer(self, simple_particle, config_with_history, adapter_with_history):
        """Should push to history buffer even during warmup."""
        model = MockModel(dt_value=1e-6)
        optimizer = torch.optim.Adam([torch.nn.Parameter(torch.zeros(1))], lr=0.01)
        hb = adapter_with_history.history_buffer

        initial_len = len(hb._buf)

        trajectory, epoch_metrics = collect_trajectory(
            model, simple_particle, optimizer, config_with_history, adapter_with_history,
            history_buffer=hb,
        )

        # Buffer should have been updated
        assert len(hb._buf) >= initial_len

    def test_returns_valid_metrics(self, simple_particle, config, adapter):
        """Epoch metrics should have expected keys."""
        model = MockModel(dt_value=1e-6)
        optimizer = torch.optim.Adam([torch.nn.Parameter(torch.zeros(1))], lr=0.01)

        _, epoch_metrics = collect_trajectory(
            model, simple_particle, optimizer, config, adapter
        )

        assert 'total_steps' in epoch_metrics
        assert 'warmup_discarded' in epoch_metrics
        assert 'trajectory_length' in epoch_metrics
        assert 'mean_retrain_iterations' in epoch_metrics
        assert 'mean_energy_error' in epoch_metrics
```
  </action>
  <verify>
Run: `cd /u/gkerex/projects/AITimeStepper && python -m pytest tests/test_trajectory_collection.py -v --tb=short 2>&1 | head -50`
  </verify>
  <done>Orchestration function tests added</done>
</task>

</tasks>

<verification>
1. Run all tests: `pytest tests/test_trajectory_collection.py -v`
2. Check coverage: `pytest tests/test_trajectory_collection.py --cov=src.trajectory_collection`
3. Verify no import errors: `python -c "import tests.test_trajectory_collection"`
</verification>

<success_criteria>
1. Test file exists at `tests/test_trajectory_collection.py`
2. All test classes and methods run without import errors
3. Tests cover:
   - attempt_single_step return structure and clone behavior
   - check_energy_threshold accept/reject logic
   - compute_single_step_loss scalar output
   - collect_trajectory_step retrain behavior
   - collect_trajectory warmup discard and history update
4. pytest exits with success (all tests pass)
</success_criteria>

<output>
After completion, create `.planning/phases/03-trajectory-collection-loop/03-04-SUMMARY.md`
</output>
