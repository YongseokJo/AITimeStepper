---
phase: 04-generalization-training-loop
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/generalization_training.py
  - src/__init__.py
autonomous: true

must_haves:
  truths:
    - "Minibatch samples random indices from trajectory list"
    - "Each sample gets single-step dt prediction and integration"
    - "Convergence loop trains until all samples pass energy threshold"
    - "Max iteration safety limit prevents infinite loops"
    - "Trajectory particles are never modified during replay (immutability)"
  artifacts:
    - path: "src/generalization_training.py"
      provides: "Generalization training functions"
      exports: ["generalize_on_trajectory", "sample_minibatch", "evaluate_minibatch"]
      min_lines: 120
    - path: "src/__init__.py"
      provides: "Module exports"
      contains: "generalize_on_trajectory"
  key_links:
    - from: "src/generalization_training.py"
      to: "src/trajectory_collection.py"
      via: "import attempt_single_step, check_energy_threshold, compute_single_step_loss"
      pattern: "from .trajectory_collection import"
    - from: "src/generalization_training.py"
      to: "src/config.py"
      via: "Config parameters"
      pattern: "config\\.replay_batch_size|config\\.replay_steps|config\\.energy_threshold"
---

<objective>
Implement the generalization training loop (Part 2) that trains on random minibatches from collected trajectory until all samples pass the energy threshold.

Purpose: This completes the second phase of the two-phase training routine. Part 1 (Phase 3) collects validated trajectory samples; Part 2 trains the model to generalize by replaying those samples until convergence.

Output: `src/generalization_training.py` module with `generalize_on_trajectory()` and supporting functions.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-generalization-training-loop/04-RESEARCH.md

# Phase 3 built these primitives we'll reuse:
@src/trajectory_collection.py
@src/config.py
@src/model_adapter.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create generalization_training.py with minibatch sampling</name>
  <files>src/generalization_training.py</files>
  <action>
Create new file `src/generalization_training.py` with:

1. **Module docstring** explaining Part 2 generalization training

2. **Imports:**
   - `import random` for minibatch sampling
   - `from typing import Any, Dict, List, Optional, Tuple`
   - `import torch`
   - `from .config import Config`
   - `from .model_adapter import ModelAdapter`
   - `from .particle import ParticleTorch`
   - `from .history_buffer import HistoryBuffer`
   - `from .trajectory_collection import attempt_single_step, check_energy_threshold, compute_single_step_loss`

3. **`sample_minibatch()` function:**
   ```python
   def sample_minibatch(
       trajectory: List[Tuple[ParticleTorch, float]],
       batch_size: int,
   ) -> List[Tuple[ParticleTorch, float]]:
       """Sample random minibatch from trajectory.

       Args:
           trajectory: List of (particle, dt) tuples from collect_trajectory()
           batch_size: Number of samples to draw (capped at trajectory length)

       Returns:
           List of (particle, dt) tuples
       """
       batch_size = min(batch_size, len(trajectory))
       return random.sample(trajectory, k=batch_size)
   ```

4. **`evaluate_minibatch()` function:**
   ```python
   def evaluate_minibatch(
       model: torch.nn.Module,
       minibatch: List[Tuple[ParticleTorch, float]],
       config: Config,
       adapter: ModelAdapter,
       history_buffer: Optional[HistoryBuffer] = None,
   ) -> Tuple[bool, List[torch.Tensor], List[torch.Tensor], Dict[str, Any]]:
       """Evaluate all samples in minibatch with single-step predictions.

       Implements TRAIN-07: Single timestep predictions per sample.

       Note: This function clones particles internally via attempt_single_step(),
       preserving immutability of the trajectory samples. The original particles
       in the minibatch are never modified.

       Args:
           model: Neural network for dt prediction
           minibatch: List of (particle, dt) tuples to evaluate
           config: Config with energy_threshold
           adapter: ModelAdapter for feature construction
           history_buffer: Optional history buffer (typically None for Part 2)

       Returns:
           (all_pass, losses, rel_dE_list, metrics) where:
           - all_pass: True if all samples passed energy threshold
           - losses: List of loss tensors for failed samples (empty if all_pass)
           - rel_dE_list: List of relative energy error tensors
           - metrics: Dict with 'pass_count', 'fail_count', 'mean_rel_dE', 'max_rel_dE'
       """
   ```

   Implementation:
   - Loop over each (particle, dt) in minibatch
   - Call `attempt_single_step(model, particle, config, adapter, history_buffer)` for each
     (Note: attempt_single_step internally calls clone_detached() on the particle)
   - Call `check_energy_threshold(E0, E1, config.energy_threshold)` for each
   - If failed, call `compute_single_step_loss(E0, E1, config)` and append to losses
   - Track all rel_dE values for metrics
   - Return (all_pass, losses, rel_dE_list, metrics)

5. **`generalize_on_trajectory()` main function:**
   ```python
   def generalize_on_trajectory(
       model: torch.nn.Module,
       trajectory: List[Tuple[ParticleTorch, float]],
       optimizer: torch.optim.Optimizer,
       config: Config,
       adapter: ModelAdapter,
       history_buffer: Optional[HistoryBuffer] = None,
   ) -> Tuple[bool, int, Dict[str, Any]]:
       """Train on random minibatches until all samples pass energy threshold.

       Implements TRAIN-05 (random minibatch), TRAIN-06 (train until all pass),
       TRAIN-07 (single timestep predictions).

       Convergence semantics:
       - "all_pass" means all samples IN THE CURRENT MINIBATCH passed the threshold
       - This is per-minibatch convergence, not trajectory-wide convergence
       - Due to random sampling, eventual convergence implies the model has learned
         to handle the full trajectory distribution (probabilistic coverage)

       Args:
           model: Neural network for dt prediction
           trajectory: List of (particle, dt) from collect_trajectory()
           optimizer: PyTorch optimizer
           config: Config with replay_batch_size, replay_steps, energy_threshold
           adapter: ModelAdapter for feature construction
           history_buffer: Optional history buffer

       Returns:
           (converged, iteration_count, metrics) where:
           - converged: True if all samples passed threshold
           - iteration_count: Number of training iterations performed
           - metrics: Dict with 'mean_rel_dE', 'max_rel_dE', 'final_pass_rate'
       """
   ```

   Implementation:
   - Handle empty trajectory edge case: return (True, 0, {}) immediately
   - Handle trajectory smaller than min_replay_size: return (True, 0, {})
   - Main loop: `for iteration in range(config.replay_steps):`
     - Sample minibatch: `minibatch = sample_minibatch(trajectory, config.replay_batch_size)`
     - Evaluate: `all_pass, losses, rel_dE_list, eval_metrics = evaluate_minibatch(...)`
     - If `all_pass`: return (True, iteration + 1, final_metrics)
       (all_pass = True when all samples in current minibatch passed, not entire trajectory)
     - Aggregate losses: `total_loss = torch.stack(losses).mean()`
     - Backprop: `optimizer.zero_grad(); total_loss.backward(); optimizer.step()`
   - If loop completes without convergence: return (False, config.replay_steps, final_metrics)

Key patterns:
- Use `random.sample()` NOT `torch.randperm()` (trajectory is Python list)
- Handle batch_size > len(trajectory) gracefully with min()
- Use config.replay_batch_size (default 512) and config.replay_steps (default 1000)
- Check config.min_replay_size (default 2) - skip if trajectory too small
  </action>
  <verify>
```bash
python -c "from src.generalization_training import generalize_on_trajectory, sample_minibatch, evaluate_minibatch; print('Imports OK')"
python -c "from src.generalization_training import sample_minibatch; print(sample_minibatch.__doc__[:50])"
```
  </verify>
  <done>
- generalize_on_trajectory exists with correct signature
- sample_minibatch samples random items from trajectory list
- evaluate_minibatch processes each sample with single-step prediction
- All functions have docstrings and type hints
- Convergence semantics clearly documented (per-minibatch, not trajectory-wide)
  </done>
</task>

<task type="auto">
  <name>Task 2: Export functions from src/__init__.py</name>
  <files>src/__init__.py</files>
  <action>
Add import statement to `src/__init__.py`:

```python
from .generalization_training import (
    generalize_on_trajectory,
    sample_minibatch,
    evaluate_minibatch,
)
```

Place after the existing trajectory_collection import block.
  </action>
  <verify>
```bash
python -c "from src import generalize_on_trajectory, sample_minibatch, evaluate_minibatch; print('Exports OK')"
```
  </verify>
  <done>
- All three functions exported from src package
- Import statement syntax correct
  </done>
</task>

<task type="auto">
  <name>Task 3: Add typing and edge case handling</name>
  <files>src/generalization_training.py</files>
  <action>
Enhance `generalize_on_trajectory()` with edge case handling:

1. **Empty trajectory:**
   ```python
   if not trajectory:
       return True, 0, {'mean_rel_dE': 0.0, 'max_rel_dE': 0.0, 'final_pass_rate': 1.0}
   ```

2. **Trajectory below min_replay_size:**
   ```python
   if len(trajectory) < config.min_replay_size:
       return True, 0, {'mean_rel_dE': 0.0, 'max_rel_dE': 0.0, 'final_pass_rate': 1.0, 'skipped': True}
   ```

3. **Metrics aggregation - explicit rel_dE computation:**
   Track across iterations. In evaluate_minibatch, aggregate rel_dE values explicitly:
   ```python
   # rel_dE_list is List[torch.Tensor], each a scalar tensor
   rel_dE_values = [dE.item() for dE in rel_dE_list]  # Convert to Python floats
   mean_rel_dE = sum(rel_dE_values) / len(rel_dE_values) if rel_dE_values else 0.0
   max_rel_dE = max(rel_dE_values) if rel_dE_values else 0.0
   ```

   Final metrics from last iteration:
   - `mean_rel_dE`: Mean of all rel_dE values from last iteration
   - `max_rel_dE`: Maximum rel_dE from last iteration
   - `final_pass_rate`: Fraction of samples passing in last iteration

4. **Ensure complete type annotations:**
   - All function parameters have types
   - All return types specified
   - Use `Optional[]` where appropriate
  </action>
  <verify>
```bash
python -c "
from src.generalization_training import generalize_on_trajectory
import inspect
sig = inspect.signature(generalize_on_trajectory)
print('Signature:', sig)
print('Return annotation:', sig.return_annotation)
"
```
  </verify>
  <done>
- Empty trajectory returns immediately with converged=True
- Small trajectory (< min_replay_size) returns with skipped=True
- All functions have complete type annotations
- Metrics dict includes mean_rel_dE, max_rel_dE, final_pass_rate
- rel_dE aggregation uses explicit `.item()` conversion and Python list operations
  </done>
</task>

</tasks>

<verification>
```bash
# Syntax check
python3 -m py_compile src/generalization_training.py && echo "Syntax OK"

# Import check
python -c "from src import generalize_on_trajectory; print('Import OK')"

# Signature check
python -c "
from src.generalization_training import generalize_on_trajectory
import inspect
sig = inspect.signature(generalize_on_trajectory)
params = list(sig.parameters.keys())
assert 'model' in params
assert 'trajectory' in params
assert 'optimizer' in params
assert 'config' in params
assert 'adapter' in params
print('Signature correct')
"

# Line count
wc -l src/generalization_training.py | awk '{if ($1 >= 120) print "Line count OK: " $1 " lines"; else print "FAIL: Only " $1 " lines"}'
```
</verification>

<success_criteria>
1. `src/generalization_training.py` exists with 120+ lines
2. `generalize_on_trajectory()` has correct signature and return type
3. `sample_minibatch()` uses `random.sample()` for random selection
4. `evaluate_minibatch()` calls Phase 3 primitives for single-step evaluation
5. All three functions exported from `src/__init__.py`
6. Edge cases handled: empty trajectory, small trajectory
7. Convergence loop respects `config.replay_steps` limit
8. Particle immutability preserved (clone_detached in attempt_single_step)
9. rel_dE metrics aggregated with explicit `.item()` conversion
</success_criteria>

<output>
After completion, create `.planning/phases/04-generalization-training-loop/04-01-SUMMARY.md`
</output>
