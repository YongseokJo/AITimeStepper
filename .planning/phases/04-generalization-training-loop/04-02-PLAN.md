---
phase: 04-generalization-training-loop
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - tests/test_generalization_training.py
autonomous: true

must_haves:
  truths:
    - "sample_minibatch returns correct number of samples"
    - "evaluate_minibatch returns all_pass=True when all samples pass"
    - "generalize_on_trajectory converges when model is good"
    - "generalize_on_trajectory returns False when max iterations reached"
  artifacts:
    - path: "tests/test_generalization_training.py"
      provides: "Unit tests for generalization training"
      min_lines: 150
      contains: "TestGeneralizeOnTrajectory"
  key_links:
    - from: "tests/test_generalization_training.py"
      to: "src/generalization_training.py"
      via: "import generalize_on_trajectory"
      pattern: "from src.generalization_training import"
---

<objective>
Create comprehensive unit tests for the generalization training module, verifying minibatch sampling, evaluation, and convergence behavior.

Purpose: Ensure the Part 2 training loop works correctly before integrating with the full epoch structure in Phase 5.

Output: `tests/test_generalization_training.py` with test cases covering all functions and edge cases.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Module under test:
@src/generalization_training.py

# Reference test patterns from Phase 3:
@tests/test_trajectory_collection.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test file with fixtures and mock models</name>
  <files>tests/test_generalization_training.py</files>
  <action>
Create `tests/test_generalization_training.py` with:

1. **Module docstring:**
   ```python
   """
   Tests for generalization training functions.

   Covers:
   - sample_minibatch
   - evaluate_minibatch
   - generalize_on_trajectory
   """
   ```

2. **Imports:**
   ```python
   import pytest
   import torch
   import torch.nn as nn

   from src.config import Config
   from src.particle import ParticleTorch
   from src.model_adapter import ModelAdapter
   from src.generalization_training import (
       generalize_on_trajectory,
       sample_minibatch,
       evaluate_minibatch,
   )
   ```

3. **Mock models (reuse patterns from test_trajectory_collection.py):**
   ```python
   class MockModel(nn.Module):
       """Mock model that returns configurable dt values.
       Dynamically adjusts input dimension based on first forward pass.
       """
       def __init__(self, dt_value: float = 0.001, input_dim: int = 11):
           super().__init__()
           self.dt_value = dt_value
           self.linear = nn.Linear(input_dim, 2, dtype=torch.float64)
           nn.init.constant_(self.linear.weight, 0.0)
           nn.init.constant_(self.linear.bias, dt_value)

       def forward(self, x):
           x = x.to(dtype=self.linear.weight.dtype)
           if x.shape[-1] != self.linear.in_features:
               new_linear = nn.Linear(x.shape[-1], 2, dtype=torch.float64, device=x.device)
               nn.init.constant_(new_linear.weight, 0.0)
               nn.init.constant_(new_linear.bias, self.dt_value)
               self.linear = new_linear
           return torch.abs(self.linear(x)) + 1e-9

   class TrainableMockModel(nn.Module):
       """Mock model with trainable parameters."""
       # Same pattern as test_trajectory_collection.py
   ```

4. **Fixtures:**
   ```python
   @pytest.fixture
   def simple_particle():
       """Create a simple 2-body particle system."""
       pos = torch.tensor([[0.5, 0.0], [-0.5, 0.0]], dtype=torch.float64)
       vel = torch.tensor([[0.0, 1.0], [0.0, -1.0]], dtype=torch.float64)
       mass = torch.tensor([1.0, 1.0], dtype=torch.float64)
       return ParticleTorch.from_tensors(mass=mass, position=pos, velocity=vel, dt=0.001)

   @pytest.fixture
   def config():
       """Create a test configuration."""
       return Config(
           energy_threshold=0.1,  # 10% for fast testing
           replay_batch_size=4,
           replay_steps=100,
           min_replay_size=2,
           E_lower=1e-6,
           E_upper=1e-2,
           num_particles=2,
           dim=2,
       )

   @pytest.fixture
   def adapter(config):
       return ModelAdapter(config, device=torch.device("cpu"), dtype=torch.float64)

   @pytest.fixture
   def sample_trajectory(simple_particle):
       """Create a sample trajectory for testing."""
       trajectory = []
       for i in range(5):
           # Create slightly perturbed particles
           p = simple_particle.clone_detached()
           trajectory.append((p, 0.001 * (i + 1)))
       return trajectory
   ```
  </action>
  <verify>
```bash
python -c "import tests.test_generalization_training; print('Test file imports OK')"
```
  </verify>
  <done>
- Test file created with correct imports
- MockModel and TrainableMockModel classes defined
- Fixtures for particle, config, adapter, and sample trajectory
  </done>
</task>

<task type="auto">
  <name>Task 2: Add tests for sample_minibatch and evaluate_minibatch</name>
  <files>tests/test_generalization_training.py</files>
  <action>
Add test classes:

```python
class TestSampleMinibatch:
    """Tests for sample_minibatch function."""

    def test_returns_correct_size(self, sample_trajectory):
        """Should return requested batch_size samples."""
        result = sample_minibatch(sample_trajectory, batch_size=3)
        assert len(result) == 3

    def test_caps_at_trajectory_length(self, sample_trajectory):
        """Should cap batch_size at trajectory length."""
        result = sample_minibatch(sample_trajectory, batch_size=100)
        assert len(result) == len(sample_trajectory)

    def test_returns_valid_tuples(self, sample_trajectory):
        """Each sample should be (ParticleTorch, float) tuple."""
        result = sample_minibatch(sample_trajectory, batch_size=2)
        for particle, dt in result:
            assert isinstance(particle, ParticleTorch)
            assert isinstance(dt, float)

    def test_random_selection(self, sample_trajectory):
        """Multiple calls should return different samples (probabilistic)."""
        results = [sample_minibatch(sample_trajectory, batch_size=2) for _ in range(10)]
        # Check that we got at least 2 different selections
        unique = set()
        for r in results:
            unique.add(tuple(dt for _, dt in r))
        assert len(unique) > 1, "Should return different random samples"


class TestEvaluateMinibatch:
    """Tests for evaluate_minibatch function."""

    def test_returns_correct_structure(self, sample_trajectory, config, adapter):
        """Should return (all_pass, losses, rel_dE_list, metrics)."""
        model = MockModel(dt_value=1e-7)  # Very small dt for quick pass
        minibatch = sample_trajectory[:2]

        result = evaluate_minibatch(model, minibatch, config, adapter)

        assert len(result) == 4
        all_pass, losses, rel_dE_list, metrics = result
        assert isinstance(all_pass, bool)
        assert isinstance(losses, list)
        assert isinstance(rel_dE_list, list)
        assert isinstance(metrics, dict)

    def test_all_pass_with_small_dt(self, sample_trajectory, config, adapter):
        """Should return all_pass=True with very small dt."""
        model = MockModel(dt_value=1e-8)  # Very small dt
        minibatch = sample_trajectory[:2]

        all_pass, losses, rel_dE_list, metrics = evaluate_minibatch(
            model, minibatch, config, adapter
        )

        assert all_pass is True
        assert len(losses) == 0  # No failures

    def test_metrics_keys(self, sample_trajectory, config, adapter):
        """Metrics should contain expected keys."""
        model = MockModel(dt_value=1e-7)
        minibatch = sample_trajectory[:2]

        _, _, _, metrics = evaluate_minibatch(model, minibatch, config, adapter)

        assert 'pass_count' in metrics
        assert 'fail_count' in metrics
        assert 'mean_rel_dE' in metrics
        assert 'max_rel_dE' in metrics
```
  </action>
  <verify>
```bash
pytest tests/test_generalization_training.py::TestSampleMinibatch -v --tb=short
pytest tests/test_generalization_training.py::TestEvaluateMinibatch -v --tb=short
```
  </verify>
  <done>
- TestSampleMinibatch with 4 test cases
- TestEvaluateMinibatch with 3 test cases
- All tests pass
  </done>
</task>

<task type="auto">
  <name>Task 3: Add tests for generalize_on_trajectory</name>
  <files>tests/test_generalization_training.py</files>
  <action>
Add test class for main function:

```python
class TestGeneralizeOnTrajectory:
    """Tests for generalize_on_trajectory function."""

    def test_returns_correct_structure(self, sample_trajectory, config, adapter):
        """Should return (converged, iteration_count, metrics)."""
        model = TrainableMockModel(input_dim=11, initial_dt=1e-7)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        result = generalize_on_trajectory(
            model, sample_trajectory, optimizer, config, adapter
        )

        assert len(result) == 3
        converged, iteration_count, metrics = result
        assert isinstance(converged, bool)
        assert isinstance(iteration_count, int)
        assert isinstance(metrics, dict)

    def test_converges_with_good_model(self, sample_trajectory, config, adapter):
        """Should converge quickly with model that produces small dt."""
        model = MockModel(dt_value=1e-8)  # Very small dt = always passes
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        converged, iterations, metrics = generalize_on_trajectory(
            model, sample_trajectory, optimizer, config, adapter
        )

        assert converged is True
        assert iterations == 1  # Should converge on first iteration

    def test_empty_trajectory_returns_converged(self, config, adapter):
        """Empty trajectory should return immediately with converged=True."""
        model = MockModel(dt_value=0.1)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        empty_trajectory = []

        converged, iterations, metrics = generalize_on_trajectory(
            model, empty_trajectory, optimizer, config, adapter
        )

        assert converged is True
        assert iterations == 0

    def test_small_trajectory_skips(self, simple_particle, config, adapter):
        """Trajectory smaller than min_replay_size should skip."""
        model = MockModel(dt_value=0.1)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        small_trajectory = [(simple_particle, 0.001)]  # Only 1 sample

        # Config has min_replay_size=2
        converged, iterations, metrics = generalize_on_trajectory(
            model, small_trajectory, optimizer, config, adapter
        )

        assert converged is True
        assert iterations == 0
        assert metrics.get('skipped', False) is True

    def test_respects_max_iterations(self, sample_trajectory, config, adapter):
        """Should stop at replay_steps if not converged."""
        # Create config with very few replay steps
        config_limited = Config(
            energy_threshold=1e-10,  # Impossible to achieve
            replay_batch_size=4,
            replay_steps=5,  # Only 5 iterations
            min_replay_size=2,
            E_lower=1e-6,
            E_upper=1e-2,
            num_particles=2,
            dim=2,
        )
        model = TrainableMockModel(input_dim=11, initial_dt=0.1)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

        converged, iterations, metrics = generalize_on_trajectory(
            model, sample_trajectory, optimizer, config_limited, adapter
        )

        assert converged is False
        assert iterations == config_limited.replay_steps

    def test_metrics_populated(self, sample_trajectory, config, adapter):
        """Metrics should be populated after training."""
        model = MockModel(dt_value=1e-8)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

        converged, iterations, metrics = generalize_on_trajectory(
            model, sample_trajectory, optimizer, config, adapter
        )

        assert 'mean_rel_dE' in metrics
        assert 'max_rel_dE' in metrics
        assert 'final_pass_rate' in metrics
```
  </action>
  <verify>
```bash
pytest tests/test_generalization_training.py::TestGeneralizeOnTrajectory -v --tb=short
```
  </verify>
  <done>
- TestGeneralizeOnTrajectory with 6 test cases
- Tests cover: return structure, convergence, empty trajectory, small trajectory, max iterations, metrics
- All tests pass
  </done>
</task>

</tasks>

<verification>
```bash
# Run all tests
pytest tests/test_generalization_training.py -v --tb=short

# Check line count
wc -l tests/test_generalization_training.py | awk '{if ($1 >= 150) print "Line count OK: " $1 " lines"; else print "FAIL: Only " $1 " lines"}'

# Ensure no test failures
pytest tests/test_generalization_training.py --tb=short -q
```
</verification>

<success_criteria>
1. `tests/test_generalization_training.py` exists with 150+ lines
2. TestSampleMinibatch class with 4+ test methods
3. TestEvaluateMinibatch class with 3+ test methods
4. TestGeneralizeOnTrajectory class with 6+ test methods
5. All tests pass with pytest
6. Tests cover edge cases: empty trajectory, small trajectory, max iterations
</success_criteria>

<output>
After completion, create `.planning/phases/04-generalization-training-loop/04-02-SUMMARY.md`
</output>
